[
  {
    "objectID": "posts/timnit-gebru-blog-post.html",
    "href": "posts/timnit-gebru-blog-post.html",
    "title": "Dr. Timnit Gebru",
    "section": "",
    "text": "In my time at Middlebury College, I have been lucky enough to take two classes that focus on ethics as they pertain to technology: Gender, Technology and Future with Professor Gupta and Politics of Virtual Realities with Professor Stanger. Readings either about or by Dr. Timnit Gebru were included in both classes’ curricula. Dr. Gebru is a computer scientist, an Ethiopian refugee who found political but not emotional asylum in the United States, and someone who has single-handedly pushed research on the ethics of Artificial Intelligence into entirely novel waters.\nDr. Gebru’s current work is focused on the Distributed Artificial Intelligence Research Institute (or DAIR), a collective of multidisciplinary researchers who examine the outcomes of AI technology, particularly as it pertains to the African continent and African immigrants in the United States. She previously acted as Google’s co-lead of the Ethical Artificial Intelligence Team, a position which ended in contention when Google asked Dr. Gebru to not publish a paper examining the dangers of bias in large language models. Dr. Gebru claims she was fired; Google claims she resigned. Either way, Google faced extensive internal and external criticism in response.\nDr. Gebru will be virtually visiting Middlebury College to give a lecture on bias and the social impacts of artificial intelligence, and more narrowly, will be visiting our class for a Q&A on Monday, April 24.\n\n\n\nDr. Gebru’s talk at the conference on Computer Vision and Pattern Recognition 2020 focuses on aspects of bias in artificial intelligence less explored by many discussing the topic. I find her idea of a dominant group close to the money very interesting; it follows a theme I noticed in articles about Dr. Gebru, that she focuses on the power dynamics of AI rather than just the biases. She talks about how visibility isn’t inclusion, which can translate to an understanding that there is bias does not mean those biases disappear.\nIt is easy for companies like Google or Amazon or Microsoft to put out a statement saying “we understand there is bias in our algorithms and datasets. We are working to diversify our datasets and hone our algorithms.” Doing the work is much more difficult and multifaceted. Dr. Gebru explains this very well, particularly in an example of Google attempting to diversify their facial recognition datasets. In doing so, Google put out advertisements asking for darker skinned people to join their dataset in a predatory manner. In a similar way, when developers came to realize gender recognition technology isn’t trained on trans people, they scraped YouTube for images of trans creators without notifying said creators. Furthermore, Dr. Gebru argues this harm towards marginalized people goes even deeper. Why is there a gender recognition system that categorizes based on a binary, socially-constructed idea anyways?\nThe point is it takes a lot of work and time to understand the implications of different technologies. In a competitive, for-profit, industry, work and time are only worth cutting. Why would a company spend the time and resources hiring experts on biases and social implications of a technology when they could make millions of dollars and cut costs simply by sending the product to the public? To harness the power dynamic between for-profit corporations and marginalized individuals, educated experts and resources need to focus on social implications of technology. It is likely that ensuring corporations take the time and resources to hire experts will require government intervention.\ntl;dr Visibility isn’t inclusion, acknowledging biases and inequity in technology development does not solve the problem of said biases and inequity, it takes much more work and depth of research into implications of technology.\n\n\n\nI have a few questions for Dr. Gebru, one which has plagued me since I wrote a paper for the Politics of Virtual Realities and one which is simply a curiosity.\n\nQuote from Meredith Whittaker, the senior advisor on AI to the Federal Trade Commission: “What I am concerned about is the capacity for social control that [AI] gives to a few profit-driven corporations.” Question: Do you think the government has the capacity to regulate the power dynamics between massive for-profit tech corporations and the individual citizen, particularly marginalized citizens? Would this have to be an international institution, or is it feasible for individual governments to have different regulations for tech corporations?\nDo you think if there were an industry-wide oath that all technologists should take, similar to the hippocratic oath, it could help mitigate some of the issues we see in technology? If yes, what would be in that oath?\n\n\n\n\nTo avoid re-enchantment with AI and to retain our human dignity and autonomy, government leaders must take initiative in discussing and questioning how AI fits into our current world; AI gone unchecked will not follow moral or ethical guidelines necessary in decision-making, particularly when it comes to governance. This is not a simple task, particularly at a time where competition is so fierce between the United States and China, two technological and economic superpowers. However, without this discussion, humans at all levels of life will promote AI to the superior thinker in our world. In doing so, humanity will give up its autonomy and dignity. Once artificial intelligence begins making decisions for humans and humans stop questioning the validity or ethical implications of said decisions, regaining human autonomy will be impossible. As Heidegger argues, the questioning of the essence of technology is necessary to avoid becoming a standing-reserve and to continue humanity’s progression. This questioning must start with world leaders, who have the experts and means available to understand the implications of artificial intelligence on us as human beings."
  },
  {
    "objectID": "posts/limits_essay_blog/limits_quantitative_methods_essay_blog.html",
    "href": "posts/limits_essay_blog/limits_quantitative_methods_essay_blog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "References\n\nGroos, Maya, Maeve Wallace, Rachel Hardeman, and Katherine P. Theall. n.d. “Measuring Inequity: A Systematic Review of Methods Used to Quantify Structural Racism.” Journal of Health Disparities Research and Practice 11 (2). https://digitalscholarship.unlv.edu/jhdrp/vol11/iss2/13/ .\n\n\nHardeman, Rachel R., Patricia A. Homan, Tongtan Chantarat, Brigette A. Davis, and Tyson H. Brown. 2022. “Improving the Measurement of Structural Racism to Achieve Antiracist Health Policy.” Health Affairs 41 (2): 179–86. https://doi.org/10.1377/hlthaff.2021.01489.\n\n\nMcCann, Edward, and Michael Brown. 2017. “Discrimination and Resilience and the Needs of People Who Identify as Transgender: A Narrative Review of Quantitative Research Studies.” Journal of Clinical Nursing 26 (23-24): 4080–93. https://doi.org/https://doi.org/10.1111/jocn.13913.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discriminatino.” https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf.\n\n\nRagin, Charles C., Susan E. Mayer, and Kriss A. Drass. 1984. “Assessing Discrimination: A Boolean Approach.” American Sociological Review 49 (2): 221–34. http://www.jstor.org/stable/2095572.\n\n\nVolpe, Vanessa V., Dalal Katsiaficas, Perusi G. Benson, and Susana N. Zelaya Rivera. n.d. “A Mixed Methods Investigation of Black College-Attending Emerging Adults’ Experiences with Multilevel Racism.” American Journal of Orthopsychiatry, 687–702. https://psycnet.apa.org/record/2020-44810-001 ."
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "A link to my source code: source code here.\n\n\nImplementing the logistic regression algorithm hinged on the correct deduction of partial derivatives from the logistic loss function. My source code, source code here, makes use of several different class functions to calculate the gradient descent. I split my logistic loss and empirical risk functions into two separate functions, as well as my predict, sigmoid, and gradient calculations. I found this easiest when bringing all aspects together in the fit function. My fit function, therefore, is not doing all the heavy work. Most of the work is spread throughout several smaller functions. Stochastic gradient descent followed naturally from gradient descent, and implementation of aforementioned small functions helped when developing the stochastic_fit method.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic_regression import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nnp.seterr(all='ignore')\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Freature 2\")\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.3, max_epochs = 1000)\n\nLR.w\n\nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 1: Learning Rate is Too Large",
    "text": "Experiment 1: Learning Rate is Too Large\nWhen the learning rate is too large, there is no convergence on a minimizer (as demonstrated below).\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 2: Batch Size Changes",
    "text": "Experiment 2: Batch Size Changes\nExample 1: Batch size of 600\nA higher batch size leads to a stochastic gradient descent that is much more similar to gradient descent – converging on a minimizer slower than a smaller batch size.\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size = 600)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nExample 2: Batch size of 10\nMuch more different than the gradient descent (which makes sense, seeing as stochastic gradient descent is essentially stochastic gradient descent with batch size of len(data)). Therefore converges quicker to a minimizer.\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size=2)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experimentation Findings",
    "text": "Experimentation Findings"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Learning Rate",
    "text": "Learning Rate\nThere is a balance between finding a learning rate that finds a quick weight array to separate the data and a learning rate that is so large that it freaks out. The large alpha experiment led to no convergence, and a loss graph that demonstrates some sort of ‘freak out’. Finding the correct learning rate is important in a successful logistic regression model."
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Batch Size",
    "text": "Batch Size\nA larger batch size is more similar to gradient descent, meaning that it converges to a minimizer slower than a smaller batch size. This makes sense logically (as mentioned above), because gradient descent is stochastic gradient descent with batch size len(data)."
  },
  {
    "objectID": "posts/Palmer_Penguins_Blog.html",
    "href": "posts/Palmer_Penguins_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Throughout this blog post, I determined a model and features which achieves 100% accuracy in the Palmer Penguins Dataset. All code is contained within this notebook.\n\n\nBelow are two displayed figures, one displayed table alongside one displayed figure.\n\n\n\nFrom this aggregation (seen below), it is clear the body mass of male penguins is significantly larger than the mass of female penguins. I hypothesized as much, but was curious how mean body mass’ compared between female and male penguins. On average, male penguins are approximately 800 grams larger than female penguins. This is interesting, not for prediction of species (unless number of females and males is an indicator for species, which is something to explore later), but for prediction of gender.\n\n#one interesting displayed table w/ pandas.groupby().aggregate, discussion of figure/table\n\nX_train.groupby('Sex_FEMALE')['Body Mass (g)'].mean()\n\nSex_FEMALE\n0    4613.076923\n1    3823.214286\nName: Body Mass (g), dtype: float64\n\n\n\n\n\nThe figures below describe the relationship between Culmen Length (mm) and Body Mass (g) in the penguins found in each island. On the left, the relationship for penguins on Dream Island and Togerson Island is shown (orange being Dream Island, blue being Torgerson Island). On the right, the relationship for penguins on Biscoe Island is shown. It is clear that on Biscoe Island there is a pretty linear relationship between increasing Culmen Length and increasing Body Mass. This is curious, as it points to perhaps a more homogenous penguin species population on Biscoe Island – the relationship is consistent for the majority of penguins on the island. Looking at Dream and Torgerson Island on the left, there is almost no relationship between Culmen Length and Body Mass. This could mean a few things; there is a wide variety of penguin species on the two islands, affecting the linearity of the relationship between the two features, or the species of penguin typically found on the two islands does not increase in Body Mass with an increase in Culmen Length.\n\n# one interesting displayed figure with seaborn + discussion\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.lmplot(\ndata=X_train, \nx=\"Culmen Length (mm)\",y=\"Body Mass (g)\", col = \"Island_Biscoe\",\nhue=\"Island_Dream\")\n\n<seaborn.axisgrid.FacetGrid at 0x7ff02872e850>\n\n\n\n\n\n\n\n\nNext, I determined the model and features to use in order to reach 100% accuracy on the testing data. With a reproducible method, I found Logistic Regression and Island, Culmen Length, and Culmen Depth to achieve 100% testing accuracy.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier #max_depth parameter, controls complexity of model, use cross-valudation to find good value of parameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #parameter gamma that controls complexity, use cross-validation to select (cover a wide range of values)\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#how the data works\ntrain.head()\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n#this is just preparing the data i.e. dropping n/as \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 15N', 'Delta 13C']\n\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n     \ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\ncols_to_use = []\nbest_score = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n       # print(cols)\n        # you could train models and score them here, keeping the list of \n        # columns for the model that has the best score. \n        # \n        lr.fit(X_train[cols], y_train)\n        if lr.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"logistic regression\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        dt.fit(X_train[cols], y_train)\n        if dt.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"decision tree\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n         \n        rf.fit(X_train[cols], y_train)\n        if rf.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"random forest\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        svc.fit(X_train[cols], y_train)\n        if svc.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"svc\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n            \n        break\n    \n\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nAgain, via the reproducible model above, I found the Logistic Regression Model and features Island, Culmen Length (mm), and Culmen Depth (mm) should find an accuracy of 100% with the testing data. This is confirmed below, where the model achieves an accuracy of 100% with the test data.\n\nLR = LogisticRegression()\nLR.fit(X_train[cols_to_use], y_train)\nLR.score(X_train[cols_to_use], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.score(X_test[cols_to_use], y_test)\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\n\nAs seen below, utilizing the Logistic Regression Model alongside the features Island, Culmen Length (mm), and Culmen Depth (mm) yields an accuracy of 1.0 and perfectly splits the decision regions.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n    \n    \nplot_regions(LR, X_train[cols_to_use], y_train)"
  },
  {
    "objectID": "posts/Linear-Regression-Blog-Post/linear-regression-blog.html",
    "href": "posts/Linear-Regression-Blog-Post/linear-regression-blog.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Linear regression is a model that helps find the linear equation that best explains the correlation between variables and an outcome. In this blog post, I implement linear regression in two ways.\nThe first is analytical, in which I use the formula: \\[\\hat w = argmin_{w}L(w) = argmin_{w}\\lVert Xw - y \\rVert^2\\] This formula updates the weight according to the loss function: \\[l(\\hat y, y) = (\\hat y - y)^2\\] With this minimization equation, one can then take the gradient with respect to \\(\\hat w\\), and solve for \\(\\hat w\\) as \\[\\hat w = (X^TX)^{-1}X^Ty\\]\nThe second was with gradient descent, the formula for the gfradient being \\[\\nabla L = 2X^T(Xw - y)\\] Each weight update, the gradient multiplied by an alpha value (default alpha = 0.001) was subtracted from the weight to obtain the new weight.\nThese two implementations of linear regression are seen below and my source code can be found at source code.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nfrom linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.5013\nValidation score = 0.6025\n\n\n\nLR.w \n\narray([0.75348988, 0.27263304])\n\n\n\n\n\nObserve the weight is the same as the analytical linear regression weight.\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train) # I used the analytical formula as my default fit method\n\nLR2.w\n\narray([0.75348774, 0.27263413])\n\n\n\n\n\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nBelow I ran an experiment; what would happen if I increased the number of features to right below the number of training data points? What occurred, shown very clearly in the graph, is overfitting. As I explain below, the validation score fell far below 0.0 with a significant increase in number of features.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores = []\nval_scores = []\np_features_array = []\n\nwhile p_features <= (n_train - 1):\n    p_features_array.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(X_train, y_train))\n    val_scores.append(LR.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array, val_scores)\nfig = plt.scatter(p_features_array, training_scores)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n# # plot it\n# fig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n# axarr[0].scatter(X_train, y_train)\n# axarr[1].scatter(X_val, y_val)\n# labs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\n# labs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n# plt.tight_layout()\n\n<matplotlib.legend.Legend at 0x7f9b0167f580>\n\n\n\n\n\nThe above observation of training and validation data demonstrates that increasing the number of features does not always help when it comes to validation data – the validation scores clearly fall off significantly as features expand, although not until the number of features near the value of n_train. This is a clear example of overfitting, when the training scores remain high (nearly 1.0), but the validation scores decrease below 0.0 (exemplary of a very bad model) when the number of features is almost equivalent to the number of data points. Full understanding of the training data does not translate to validation data, but rather handles only the specifics of the training data.\n\n\n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\n\nL.score(X_val, y_val)\n\n0.6076240794781351\n\n\n\n\n\nAs seen below, I demonstrate three graphs which show the validation and training scores for LASSO regularized linear regression models with three different alpha values. In all three, the validation scores decrease, but not nearly with the same drastic change as with regular linear regression.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.001)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n<matplotlib.legend.Legend at 0x7f9b00d88430>\n\n\n\n\n\nWith LASSO regulation, when the number of features gets large the validation scores still drop. However, they remain higher than with normal linear regression. They still fall off significantly when number of feature increase, but they do not fall as far as the linear regression models did (i.e. on this specific data, LASSO falls to a little under 0.5 validation score whereas linear regression falls under 0.0 validation score). This demonstrates how LASSO regularization helps overparametized models – it helps protect the model (at least slightly) from overfitting.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.01)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n<matplotlib.legend.Legend at 0x7f9b018f7280>\n\n\n\n\n\nChanging the alpha (i.e. making the alpha value 0.01 instead of 0.001) changes the validation scores even more significantly. The validation scores fall off much earlier – around p_features = 50 they start decreasing – but they still do not fall as low as the linear regression validation scores (they reach the same score as with alpha = 0.001).\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.0001)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e-02, tolerance: 4.368e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e-02, tolerance: 3.745e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e-01, tolerance: 4.979e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.573e-02, tolerance: 4.222e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n<matplotlib.legend.Legend at 0x7f9b01b55a60>\n\n\n\n\n\nOn the other hand, decreasing the alpha from 0.001 to 0.0001 makes the validation scores decrease much slower and to a much lesser extent. In the above graph, the validation scores with Lasso regularization only fall to around 0.75. Decreasing the alpha, therefore, increases the validation scores."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Here is a link to my source code on the GitHub repo perceptron.py.\nThe perceptron update is done in my perceptron.fit() function, in which I begin by modifying the features matrix X by adding a row of ones to the features matrix to ensure that the bias is taken into account in the update (as w tilde is a vector of the weights and the bias). The function begins by assigning a random weight vector, then entering into a loop as long as the maximum steps. In this loop, a random index is chosen and then the point from that index as well as its feature vector are entered into a weight update equation which updates the weight if the predicted label and actual label are different. This for loop also keeps track of the score (accuracy) throughout the iterations."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Linearly Separable",
    "text": "Experiment: Linearly Separable\nThe first experiment I ran had to do with linearly separable data, to ensure that the line converges. I created a perceptron object and ran my perceptron.fit function on that object, and the line did end up converging. I printed out my accuracies, and they fluctuated (albeit barely). The accuracy did end up reaching 1.0, proving line convergence. This code (and output) can be observed below.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Not Linearly Separable",
    "text": "Experiment: Not Linearly Separable\nThe second experiment I did had to do with data that was not linearly separable, to see if the line didn’t converge. Indeed, when the data was not linearly separable (as seen below), the line did not converge and the accuracy never reached 1.0. I utilized the make_circles function from sklearn.datasets to create my non linearly separable data, and the accuracy ended up (after max_steps equal to 1000) at 0.53.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_circles(n_samples = 100, noise = 0.5)\n\n#X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.5, 0.5, 0.5, 0.58, 0.59, 0.59, 0.59, 0.59, 0.47, 0.47, 0.47, 0.55, 0.55, 0.5, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.53, 0.53, 0.54, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.61, 0.61, 0.54, 0.54, 0.5, 0.57, 0.57, 0.57, 0.57, 0.53, 0.53, 0.53, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.56, 0.56, 0.56, 0.48, 0.48, 0.56, 0.56, 0.56, 0.5, 0.5, 0.5, 0.57, 0.49, 0.58, 0.58, 0.57, 0.57, 0.57, 0.62, 0.62, 0.62, 0.62, 0.47, 0.47, 0.47, 0.52, 0.45, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.47, 0.47, 0.47, 0.51, 0.51, 0.5, 0.45, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.45, 0.45, 0.55, 0.55, 0.53, 0.49, 0.51, 0.62, 0.62, 0.44, 0.44, 0.5, 0.44, 0.44, 0.44, 0.51, 0.51, 0.46, 0.47, 0.51, 0.51, 0.51, 0.51, 0.58, 0.49, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.54, 0.54, 0.49, 0.49, 0.56, 0.56, 0.56, 0.49, 0.49, 0.49, 0.49, 0.56, 0.55, 0.6, 0.51, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.53, 0.51, 0.51, 0.5, 0.43, 0.43, 0.43, 0.43, 0.51, 0.51, 0.51, 0.53, 0.49, 0.46, 0.46, 0.53, 0.53, 0.53, 0.56, 0.56, 0.52, 0.52, 0.5, 0.54, 0.54, 0.49, 0.59, 0.5, 0.5, 0.5, 0.5, 0.48, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.56, 0.56, 0.52, 0.52, 0.57, 0.57, 0.57, 0.57, 0.49, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.52, 0.47, 0.54, 0.58, 0.58, 0.58, 0.58, 0.58, 0.57, 0.57, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.57, 0.56, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.55, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.51, 0.51, 0.51, 0.51, 0.56, 0.5, 0.5, 0.53, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.52, 0.52, 0.52, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5, 0.5, 0.52, 0.55, 0.56, 0.56, 0.56, 0.56, 0.59, 0.59, 0.5, 0.5, 0.55, 0.5, 0.5, 0.58, 0.58, 0.56, 0.55, 0.55, 0.55, 0.55, 0.47, 0.53, 0.48, 0.46, 0.46, 0.45, 0.49, 0.44, 0.49, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.49, 0.49, 0.49, 0.49, 0.6, 0.6, 0.6, 0.5, 0.55, 0.5, 0.5, 0.55, 0.55, 0.5, 0.5, 0.46, 0.55, 0.55, 0.55, 0.55, 0.55, 0.49, 0.44, 0.48, 0.48, 0.48, 0.48, 0.44, 0.46, 0.43, 0.5, 0.5, 0.5, 0.5, 0.51, 0.45, 0.56, 0.52, 0.52, 0.6, 0.55, 0.55, 0.55, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.51, 0.51, 0.51, 0.58, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.47, 0.47, 0.45, 0.45, 0.45, 0.45, 0.46, 0.46, 0.46, 0.5, 0.5, 0.51, 0.49, 0.48, 0.55, 0.55, 0.51, 0.51, 0.58, 0.58, 0.55, 0.55, 0.55, 0.55, 0.5, 0.53, 0.52, 0.51, 0.51, 0.46, 0.46, 0.46, 0.46, 0.53, 0.53, 0.51, 0.51, 0.5, 0.5, 0.55, 0.5, 0.46, 0.45, 0.45, 0.43, 0.43, 0.46, 0.46, 0.46, 0.5, 0.5, 0.5, 0.5, 0.42, 0.47, 0.47, 0.47, 0.47, 0.51, 0.51, 0.51, 0.51, 0.51, 0.43, 0.54, 0.44, 0.44, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.5, 0.47, 0.46, 0.55, 0.57, 0.57, 0.57, 0.5, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.6, 0.5, 0.46, 0.46, 0.46, 0.46, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.5, 0.5, 0.5, 0.5, 0.51, 0.53, 0.53, 0.53, 0.53, 0.54, 0.54, 0.53, 0.53, 0.46, 0.46, 0.46, 0.46, 0.48, 0.5, 0.44, 0.44, 0.44, 0.51, 0.5, 0.5, 0.44, 0.48, 0.48, 0.46, 0.46, 0.47, 0.5, 0.5, 0.5, 0.49, 0.46, 0.46, 0.53, 0.45, 0.45, 0.44, 0.44, 0.49, 0.42, 0.53, 0.53, 0.47, 0.47, 0.47, 0.47, 0.47, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.6, 0.6, 0.52, 0.52, 0.57, 0.58, 0.56, 0.57, 0.59, 0.59, 0.59, 0.59, 0.59, 0.55, 0.55, 0.55, 0.55, 0.6, 0.47, 0.51, 0.56, 0.56, 0.51, 0.57, 0.55, 0.55, 0.55, 0.55, 0.54, 0.51, 0.56, 0.47, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.55, 0.46, 0.59, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.53, 0.53, 0.53, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.5, 0.5, 0.55, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.48, 0.48, 0.48, 0.48, 0.56, 0.56, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.5, 0.5, 0.49, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.5, 0.58, 0.58, 0.56, 0.56, 0.56, 0.56, 0.56, 0.57, 0.48, 0.48, 0.48, 0.48, 0.49, 0.57, 0.57, 0.57, 0.55, 0.55, 0.53, 0.53, 0.55, 0.55, 0.55, 0.5, 0.5, 0.5, 0.55, 0.55, 0.51, 0.51, 0.51, 0.56, 0.56, 0.54, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.5, 0.54, 0.5, 0.41, 0.52, 0.52, 0.54, 0.54, 0.54, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.46, 0.48, 0.47, 0.47, 0.47, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.46, 0.5, 0.53, 0.53, 0.55, 0.55, 0.55, 0.49, 0.52, 0.52, 0.46, 0.46, 0.46, 0.49, 0.54, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.55, 0.55, 0.55, 0.52, 0.52, 0.51, 0.51, 0.51, 0.49, 0.49, 0.49, 0.49, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.5, 0.5, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.45, 0.49, 0.45, 0.45, 0.41, 0.52, 0.52, 0.42, 0.42, 0.49, 0.49, 0.45, 0.52, 0.45, 0.51, 0.46, 0.46, 0.46, 0.48, 0.48, 0.47, 0.47, 0.5, 0.45, 0.45, 0.46, 0.46, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.47, 0.47, 0.52, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.51, 0.47, 0.47, 0.47, 0.52, 0.52, 0.52, 0.42, 0.5, 0.5, 0.57, 0.57, 0.57, 0.59, 0.49, 0.49, 0.49, 0.49, 0.53, 0.53, 0.53, 0.53, 0.51, 0.45, 0.46, 0.46, 0.46, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.52, 0.6, 0.6, 0.6, 0.54, 0.55, 0.54, 0.54, 0.53, 0.53, 0.51, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.57, 0.57, 0.48, 0.45, 0.42, 0.48, 0.48, 0.48, 0.53, 0.54, 0.54, 0.49, 0.49, 0.49, 0.47, 0.47, 0.5, 0.5, 0.58, 0.56, 0.56, 0.56, 0.5, 0.56, 0.53, 0.56, 0.5, 0.56, 0.56, 0.53, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.48, 0.5, 0.5, 0.5, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.48, 0.48, 0.48, 0.5, 0.5, 0.55, 0.55, 0.55, 0.51, 0.51, 0.5, 0.61, 0.55, 0.52, 0.52, 0.52, 0.52, 0.47, 0.51, 0.54, 0.5, 0.5, 0.5, 0.53]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-multiple-dimensions",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-multiple-dimensions",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Multiple Dimensions",
    "text": "Experiment: Multiple Dimensions\nThe third and final experiment has to do with the perceptron algorithm working on more than 2 dimensions. I created a data set with 5 features and then printed the score evolution. Even with multiple dimensions, the algorithm converges to an accuracy of 1.0.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Runtime Complexity of a Perceptron Update",
    "text": "Runtime Complexity of a Perceptron Update\nThe runtime complexity of a single iteration of the perceptron algorithm update is dependent on the number of data points n, but not on the number of features p (this is specific to my implementation of the algorithm, where I am looking at accuracy / adding the score to a property history for th perceptron). The runtime is going to be O(max_steps * n) where n is the number of data points. the for loop iterates through the specified number of max_steps, but everything within that for loop (other than the calculation of the score) is O(1). The calculation of the score is O(n), because the score predicts a label for each of the data points. Therefore, within the for loop, the runtime is O(n), making the total runtime O(max_steps * n)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "Palmer_Penguins_Blog.html",
    "href": "Palmer_Penguins_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Throughout this blog post, I determined a model and features which achieves 100% accuracy in the Palmer Penguins Dataset. All code is contained within this notebook.\n\n\nBelow are two displayed figures, one displayed table alongside one displayed figure.\n\n\n\nFrom this aggregation (seen below), it is clear the body mass of male penguins is significantly larger than the mass of female penguins. I hypothesized as much, but was curious how mean body mass’ compared between female and male penguins. On average, male penguins are approximately 800 grams larger than female penguins. This is interesting, not for prediction of species (unless number of females and males is an indicator for species, which is something to explore later), but for prediction of gender.\n\n#one interesting displayed table w/ pandas.groupby().aggregate, discussion of figure/table\n\nX_train.groupby('Sex_FEMALE')['Body Mass (g)'].mean()\n\nSex_FEMALE\n0    4613.076923\n1    3823.214286\nName: Body Mass (g), dtype: float64\n\n\n\n\n\nThe figures below describe the relationship between Culmen Length (mm) and Body Mass (g) in the penguins found in each island. On the left, the relationship for penguins on Dream Island and Togerson Island is shown (orange being Dream Island, blue being Torgerson Island). On the right, the relationship for penguins on Biscoe Island is shown. It is clear that on Biscoe Island there is a pretty linear relationship between increasing Culmen Length and increasing Body Mass. This is curious, as it points to perhaps a more homogenous penguin species population on Biscoe Island – the relationship is consistent for the majority of penguins on the island. Looking at Dream and Torgerson Island on the left, there is almost no relationship between Culmen Length and Body Mass. This could mean a few things; there is a wide variety of penguin species on the two islands, affecting the linearity of the relationship between the two features, or the species of penguin typically found on the two islands does not increase in Body Mass with an increase in Culmen Length.\n\n# one interesting displayed figure with seaborn + discussion\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.lmplot(\ndata=X_train, \nx=\"Culmen Length (mm)\",y=\"Body Mass (g)\", col = \"Island_Biscoe\",\nhue=\"Island_Dream\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f9c71ed0f10>\n\n\n\n\n\n\n\n\nNext, I determined the model and features to use in order to reach 100% accuracy on the testing data. With a reproducible method, I found Logistic Regression and Island, Culmen Length, and Culmen Depth to achieve 100% testing accuracy.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier #max_depth parameter, controls complexity of model, use cross-valudation to find good value of parameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #parameter gamma that controls complexity, use cross-validation to select (cover a wide range of values)\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#how the data works\ntrain.head()\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n#this is just preparing the data i.e. dropping n/as \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 15N', 'Delta 13C']\n\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n     \ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\ncols_to_use = []\nbest_score = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n       # print(cols)\n        # you could train models and score them here, keeping the list of \n        # columns for the model that has the best score. \n        # \n        lr.fit(X_train[cols], y_train)\n        if lr.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"logistic regression\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        dt.fit(X_train[cols], y_train)\n        if dt.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"decision tree\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n         \n        rf.fit(X_train[cols], y_train)\n        if rf.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"random forest\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        svc.fit(X_train[cols], y_train)\n        if svc.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"svc\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n            \n        break\n    \n\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nAgain, via the reproducible model above, I found the Logistic Regression Model and features Island, Culmen Length (mm), and Culmen Depth (mm) should find an accuracy of 100% with the testing data. This is confirmed below, where the model achieves an accuracy of 100% with the test data.\n\nLR = LogisticRegression()\nLR.fit(X_train[cols_to_use], y_train)\nLR.score(X_train[cols_to_use], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.score(X_test[cols_to_use], y_test)\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\n\nAs seen below, utilizing the Logistic Regression Model alongside the features Island, Culmen Length (mm), and Culmen Depth (mm) yields an accuracy of 1.0 and perfectly splits the decision regions.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n    \n    \nplot_regions(LR, X_train[cols_to_use], y_train)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Unsupervised Learning with Singular Value Decomposition and Laplacian Spectral Clustering\n\n\n\n\n\n\nApr 26, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA discussion of Dr. Timnit Gebru and her work.\n\n\n\n\n\n\nApr 25, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing linear regression.\n\n\n\n\n\n\nApr 5, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn essay assessing Narayanan’s statement that current quantitative methods justify the status quo, and that they do more harm than good.\n\n\n\n\n\n\nMar 30, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bridget Ulian CS0145 Blog",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html",
    "href": "posts/unsupervised_learning_blog.html",
    "title": "Unsupervised Learning Blog",
    "section": "",
    "text": "In this blog, I look at singular value decomposition as it pertains to images. Singular value decomposition is a matrix factorization that can be used to compress images, decreasing the storage needed to store an image.\nThe singular value decomposition of a real matrix A is:\n\\[ A = U*D*V^T\\]\nIn the case that D is a real, diagonal matrix and real matrices U and V are orthogonal matrices. One can use this singular value decomposition to compress images down to much smaller images by selecting a component we can call ‘k’; by only selecting the first k columns of U, the top k singular values in D, and the first k rows of V, one can approximate the original matrix A by matrix multiplying these three new matrices.\nWith smaller values of k, understandably, the reconstructed matrix will be further from the original matrix. With higher values of k, the reconstructed matrix will be more similar to the original matrix.\nBecause images themselves are matrices, one can apply singular value decomposition to images. The code below demonstrates the process.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n\n\nBelow I created a function called svd_reconstruct that uses singular value decomposition to compress an image as explained above. There is some set-up code prior to that function; a function called compare_images that allows a user to compare the original and reconstructed images, a read_image function that reads in an image from the Internet, and a to_greyscale image (with examples) to convert an image to greyscale.\n\ndef compare_images(A, A_):\n    fig, axarr = plt.subplots(1, 2, figsize = (7,3))\n    \n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n    \n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n    \n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://cdn-prd.content.metamorphosis.com/wp-content/uploads/sites/2/2020/10/shutterstock_1073056451-2.jpg\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAfter that set up, below is the actual implementation of the svd_reconstruct function alongside a brief test of an original image and reconstructed image of four components.\n\ndef svd_reconstruct(img, k):\n    # A = U D V(transpose)\n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\n\ngrey_img_2 = svd_reconstruct(grey_img, 4)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\n\n\nThe purpose of singular value decomposition in compressing images is to decrease storage space an image takes up. The function below, svd_experiment, runs through component values of 5, 15, 25, 35, 45, and 55 and explores what percentage of the original storage each image takes up. I increased the value of k until the naked eye (or at least my naked eye) could not perceive a difference between the original image and the reconstructed image. I confirmed that by using the compare_images function.\nAt 55 components, the reconstructed image only took up 13.8% of the original image’s storage space. This is a significant difference in storage for a minute, invisible-to-the-human-eye change in image structure.\n\ndef svd_experiment(img):\n\n    fig = plt.figure(figsize = (8, 8))\n    columns = 2\n    rows = 3\n    \n    k_arr = [5, 15, 25, 35, 45, 55]\n    \n    ax=[]\n    \n        \n    for i in range(0, columns*rows):\n        new_img = svd_reconstruct(img, k_arr[i])\n        \n        #storage \n        amt_storage = (k_arr[i] * (img.shape[0] + img.shape[1] + 1)) / (img.shape[0] * img.shape[1])\n        \n        ax.append(fig.add_subplot(rows, columns, i+1))\n        ax[-1].set_title(f\"{k_arr[i]} components, % storage = {(round(100 * amt_storage,1))}\")\n        \n       # fig.add_subplot(rows, columns, i)\n        plt.imshow(new_img, cmap=\"Greys\")\n        \n    plt.tight_layout()    \n    plt.show()\n        \nsvd_experiment(grey_img)\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 55))\n\n\n\n\n\n\n\nAfter finishing the svd_reconstruct, it is important to note that rarely do people understand image compression by the value of k. They are more likely to use a compression factor, how much they want the image storage to decrease by. Below I implemented a version of svd_reconstruct in which the user imputs the compression factor (i.e. 5 means the image is compressed to 1/5 the original storage space).\nThis required some extra math within the reconstruct function to determine the value of k for a given compression factor. That code can be seen below.\n\ndef svd_reconstruct_extra(img, comp_factor):\n    \n    #get the new size of the compressed image\n    new_size = (img.shape[0] * img.shape[1]) / comp_factor\n    \n    #get the k value (integer value) from the new size of the compressed image\n    k = int(new_size / (img.shape[0] + img.shape[0] + 1)) \n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\nBelow I tested two different compression factors; one very large (80), and one fairly small (10). It is clear that with a higher compression factor – the image taking up significantly less space than the original – the reconstructed image is far different than the original. With a smaller compression factor, 10, the reconstructed image is nearly the same as the original image.\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 80)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 10)\ncompare_images(grey_img, grey_img_2)"
  },
  {
    "objectID": "posts/timnit-gebru-blog-post.html#section-1-dr.-gebrus-argument",
    "href": "posts/timnit-gebru-blog-post.html#section-1-dr.-gebrus-argument",
    "title": "Dr. Timnit Gebru",
    "section": "Section 1: Dr. Gebru’s Argument",
    "text": "Section 1: Dr. Gebru’s Argument\nDr. Timnit Gebru’s argument about AGI and second-wave eugenics is this; people are fixated on AI as a means to a utopia or an apocalypse, a transhuman experience, and in doing so are not paying attention to the current problems of AGI and the fact that reaching that transhuman experience requires discriminatory choices, abusive labor, and deep wealth disparities. She draws a very clear connection between first-wave eugenics (think sterilization of the disabled and people of color, something that I learned recently in my American Psycho class continued deep into the 1960s) and second-wave eugenics. She paints both first and second wave eugenics as problematic in somewhat similar ways; both define intelligence in ways that play into casual racism and promote certain traits as positive that are typically found in wealthy, well-educated, white people. She also touched on the monopolization of AGI and the “race to the bottom” in creating larger, more generalized, more versatile models. The problem with larger models, Timnit argued, is that they do not feed back into the communities they come out of.\nSecond-wave eugenics are harmful, Dr. Gebru argues, partially because they choose what qualifies as ‘intelligent’ and what traits are desirable in humans going forward to a posthuman existence. One thing I found interesting in particular was how often second-wave eugenicists cite Charles Murray. It was clear that Dr. Gebru did not understand the history between Charles Murray and Middlebury College, but that history being as contentious as it is means this point was especially salient. If second-wave eugenicists are worried about being labeled discriminatory and racist, they should not draw a connection between themselves and a homophobic, racist, supposed scholar.\nAnother issue with the second-wave eugenicists is their treatment of AI as some mystical, magical, future superpower. Treating AI as a path toward either utopia or apocalypse takes away from the fact that AI is developing currently, under discriminatory, rushed, and vastly unfair circumstances. AGI is not a future mystical superpower, but a current ailment. In order to change the problematic foundation of Artificial Intelligence, computer scientists and billionaires should focus on how to fix the problems of today; how can we stop large models and corporations from monopolizing the market? How can we provide less abusive career paths for people labeling datasets, or moderating models, or having their images and artwork used in datasets?\nI agree in general with Dr. Gebru’s argument. I think she brings up very interesting points about posthumanism and the TESCREAL community, and I believe I will bring a lot of the context from this talk with me as I continue reading about the forays of Elon Musk and crew. I also liked that she reached beyond the technicalities of artificial intelligence models, beyond the ‘check that your data is not biased’ and ‘think about who these algorithms affect.’ I think both of these things are wildly important, but I was also searching for something new from Dr. Gebru; she definitely provided something new to chew on. I do wish, however, that she touched a little more on her interactions with legislators and her hopes for government regulation going forward. I came close to asking a question about it, but then another question was asked that provided a vague answer to what she looks for in regulation. As a political science major (as well as computer science), I am so intrigued by tech regulation in governance. I would have loved to hear what a typical talk with a member of congress sounds like for Dr. Gebru; does she explain the technicalities to each legislator? What does the current state of AI regulation look like in the government? I know that Congress was mocked endlessly for their questioning of Mark Zuckerberg; is it even possible for today’s Congress to regulate tech with their lack of expertise?"
  },
  {
    "objectID": "posts/timnit-gebru-blog-post.html#reflection",
    "href": "posts/timnit-gebru-blog-post.html#reflection",
    "title": "Dr. Timnit Gebru",
    "section": "Reflection",
    "text": "Reflection\nI loved Dr. Gebru’s talk and I think I could’ve sat there listening to her for a lot longer. She is so thoughtful in her speech, something that I greatly admire about very smart people. I appreciated her expertise in fields outside of technical computer vision and her willingness to dip into the theories behind TESCREAL. This past summer, I found myself searching endlessly for jobs that combine political science/policy and computer science. I stumbled across Schmidt Futures, Eric Schmidt’s philanthropic fellowship for computer science students, applied, and was promptly rejected. I have considered going to graduate school for tech public policy, and am very interested in trying to limit the reach of massive tech corporations. Side note, going to the former CEO of Google’s philanthropic fellowship was probably not the best way to take down big tech corporations. Hearing someone speak who is so knowledgeable in a field I have been trying to find my way into was magical. Dr. Gebru inspired the same fascination in me in just one hour that semester-long classes have inspired.\nLike I said, my search for a job at the crux of political science and computer science was rather difficult. I found myself locked into a corporate software engineering job, with hopes of going to graduate school in the future. Whenever I find the spark waning (money is a very strong pull, losing money a pretty strong push), I feel as though I can think back on Dr. Gebru’s talk. I would do pretty much anything to sit there and pick her brain for hours. Thank you Phil for emailing her to come talk to us."
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#laplacian-spectral-clustering",
    "href": "posts/unsupervised_learning_blog.html#laplacian-spectral-clustering",
    "title": "Unsupervised Learning Blog",
    "section": "Laplacian Spectral Clustering",
    "text": "Laplacian Spectral Clustering\nLaplacian Spectral Clustering, which is demonstrated below, is a clustering algorithm that works whether data is linearly separable or not. This makes it helpful in separating data that is not linearly separable, potentially like graphs or circular divisions.\nLaplacian clustering works in the following manner. First, we get the adjacency matrix of the graph. The adjacency matrix demonstrates the edges between nodes. Let’s call this adjacency matrix A. In order to determine relationships between nodes, we can consider the edges between different clusters; we want few edges that connect nodes from different clusters. Let’s call these edges connecting between edges cuts. We want to minimize the cuts between clusters.\nWe are trying to find a vector z such that the number of cuts are minimized. Finding a binary vector \\(z \\in {0,1}\\) is an NP-hard problem, but we can approximate the binary vector z by doing the following.\nA normalized Laplacian matrix can be defined as the matrix \\[ L = D^{-1}[D - A] \\]\nThe diagonal matrix D is the diagonal matrix where\n\\[  D = \\begin{bmatrix}{\\sum_{i = 1}^{n} a_{i1}} && \\\\ & \\ddots & \\\n\\ && \\sum_{i = 1}^{n} a_{in} \\end{bmatrix} \\]\nWhen we have the normalized Laplacian L, z should be the eigenvector with the second-smallest eigenvalue of the normalized Laplacian. With this binary vector, we can split the non-linear data into clusters.\nI will demonstrate this below, but begin with a little insight into the graph used in this process.\n\nThe Graph\nBelow is the graph nicknamed the “Karate Club Graph”, a graph in which each node is an individual member of a karate club. The edges are measures of social ties.\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nThe karate club ended up splitting between two officers, who we will call Mr. Hi and Officer. Knowing which club each member has gone to, we can create the graph below. It colors each node orange if that member belongs to the Officer club and blue if they belong to the Mr. Hi club.\n\nclubs = nx.get_node_attributes(G, \"club\")\n\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\nLaplacian Spectral Clustering on the Karate Club Graph\nBelow is the process for applying the Laplacian clustering method described above on the karate club graph. The implemented function ‘spectral_clustering’ returns a binary vector, z, that determines which cluster each node belongs to.\n\nfrom sklearn.neighbors import NearestNeighbors\nimport networkx as nx\n\ndef spectral_clustering(G):\n\n    A = nx.adjacency_matrix(G).toarray()\n    \n    #find D, start by getting the sums of each row of A\n    row_sums = np.sum(A, axis = 1)\n    \n    #put sums into diagonal matrix D\n    D = np.diag(row_sums)\n    \n    #find normalized Laplacian \n    L = np.linalg.inv(D)@(D - A)\n    \n    # find z, eigenvector with second smallest eigenvalue of Laplacian\n    eig_vals, eig_vecs = np.linalg.eig(L)\n\n    #sort eigenvalues, get the second smallest eigenvalue\n    vals_sorted = np.sort(eig_vals)\n    z_val = vals_sorted[1]\n    \n    #get index of second smallest eigenvalue and get corresponding eigenvector: this is z\n    ind = eig_vals.tolist().index(z_val)\n    z = eig_vecs[:,ind]\n\n    #make z binary\n    z = np.where(z > 0, 1, 0)\n    return z\n\n\n\nThe Accuracy of Laplacian Clustering\nThe Laplacian clustering did a remarkably good job of separating the nodes into their two clusters or clubs. There was one mistake, node number 8, who went with club Mr. Hi but which my Laplacian clustering algorithm placed with the Officer club. Every other node was placed in the correct cluster. It is clear that Laplacian clustering worked very well in this case.\n\nz = spectral_clustering(G)\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if z[i] == 0 else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        )"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#introduction",
    "href": "posts/unsupervised_learning_blog.html#introduction",
    "title": "Unsupervised Learning Blog",
    "section": "Introduction",
    "text": "Introduction\nIn this blog, I look at singular value decomposition as it pertains to images. Singular value decomposition is a matrix factorization that can be used to compress images, decreasing the storage needed to store an image.\nThe singular value decomposition of a real matrix A is:\n\\[ A = U*D*V^T\\]\nIn the case that D is a real, diagonal matrix and real matrices U and V are orthogonal matrices. One can use this singular value decomposition to compress images down to much smaller images by selecting a component we can call ‘k’; by only selecting the first k columns of U, the top k singular values in D, and the first k rows of V, one can approximate the original matrix A by matrix multiplying these three new matrices.\nWith smaller values of k, understandably, the reconstructed matrix will be further from the original matrix. With higher values of k, the reconstructed matrix will be more similar to the original matrix.\nBecause images themselves are matrices, one can apply singular value decomposition to images. The code below demonstrates the process.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#part-1-image-compression-with-singular-value-decompression",
    "href": "posts/unsupervised_learning_blog.html#part-1-image-compression-with-singular-value-decompression",
    "title": "Unsupervised Learning Blog",
    "section": "Part 1: Image Compression with Singular Value Decompression",
    "text": "Part 1: Image Compression with Singular Value Decompression\nBelow I created a function called svd_reconstruct that uses singular value decomposition to compress an image as explained above. There is some set-up code prior to that function; a function called compare_images that allows a user to compare the original and reconstructed images, a read_image function that reads in an image from the Internet, and a to_greyscale image (with examples) to convert an image to greyscale.\n\ndef compare_images(A, A_):\n    fig, axarr = plt.subplots(1, 2, figsize = (7,3))\n    \n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n    \n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n    \n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://cdn-prd.content.metamorphosis.com/wp-content/uploads/sites/2/2020/10/shutterstock_1073056451-2.jpg\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAfter that set up, below is the actual implementation of the svd_reconstruct function alongside a brief test of an original image and reconstructed image of four components.\n\ndef svd_reconstruct(img, k):\n    # A = U D V(transpose)\n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\n\ngrey_img_2 = svd_reconstruct(grey_img, 4)\ncompare_images(grey_img, grey_img_2)"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#experimentation",
    "href": "posts/unsupervised_learning_blog.html#experimentation",
    "title": "Unsupervised Learning Blog",
    "section": "Experimentation",
    "text": "Experimentation\nThe purpose of singular value decomposition in compressing images is to decrease storage space an image takes up. The function below, svd_experiment, runs through component values of 5, 15, 25, 35, 45, and 55 and explores what percentage of the original storage each image takes up. I increased the value of k until the naked eye (or at least my naked eye) could not perceive a difference between the original image and the reconstructed image. I confirmed that by using the compare_images function.\nAt 55 components, the reconstructed image only took up 13.8% of the original image’s storage space. This is a significant difference in storage for a minute, invisible-to-the-human-eye change in image structure.\n\ndef svd_experiment(img):\n\n    fig = plt.figure(figsize = (8, 8))\n    columns = 2\n    rows = 3\n    \n    k_arr = [5, 15, 25, 35, 45, 55]\n    \n    ax=[]\n    \n        \n    for i in range(0, columns*rows):\n        new_img = svd_reconstruct(img, k_arr[i])\n        \n        #storage \n        amt_storage = (k_arr[i] * (img.shape[0] + img.shape[1] + 1)) / (img.shape[0] * img.shape[1])\n        \n        ax.append(fig.add_subplot(rows, columns, i+1))\n        ax[-1].set_title(f\"{k_arr[i]} components, % storage = {(round(100 * amt_storage,1))}\")\n        \n       # fig.add_subplot(rows, columns, i)\n        plt.imshow(new_img, cmap=\"Greys\")\n        \n    plt.tight_layout()    \n    plt.show()\n        \nsvd_experiment(grey_img)\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 55))"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#extras-modifying-svd_reconstruct-with-compression-factors",
    "href": "posts/unsupervised_learning_blog.html#extras-modifying-svd_reconstruct-with-compression-factors",
    "title": "Unsupervised Learning Blog",
    "section": "Extras: Modifying svd_reconstruct with Compression Factors",
    "text": "Extras: Modifying svd_reconstruct with Compression Factors\nAfter finishing the svd_reconstruct, it is important to note that rarely do people understand image compression by the value of k. They are more likely to use a compression factor, how much they want the image storage to decrease by. Below I implemented a version of svd_reconstruct in which the user imputs the compression factor (i.e. 5 means the image is compressed to 1/5 the original storage space).\nThis required some extra math within the reconstruct function to determine the value of k for a given compression factor. That code can be seen below.\n\ndef svd_reconstruct_extra(img, comp_factor):\n    \n    #get the new size of the compressed image\n    new_size = (img.shape[0] * img.shape[1]) / comp_factor\n    \n    #get the k value (integer value) from the new size of the compressed image\n    k = int(new_size / (img.shape[0] + img.shape[0] + 1)) \n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\nBelow I tested two different compression factors; one very large (80), and one fairly small (10). It is clear that with a higher compression factor – the image taking up significantly less space than the original – the reconstructed image is far different than the original. With a smaller compression factor, 10, the reconstructed image is nearly the same as the original image.\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 80)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 10)\ncompare_images(grey_img, grey_img_2)"
  }
]