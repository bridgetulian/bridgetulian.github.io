[
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Here is a link to my source code on the GitHub repo perceptron.py.\nThe perceptron update is done in my perceptron.fit() function, in which I begin by modifying the features matrix X by adding a row of ones to the features matrix to ensure that the bias is taken into account in the update (as w tilde is a vector of the weights and the bias). The function begins by assigning a random weight vector, then entering into a loop as long as the maximum steps. In this loop, a random index is chosen and then the point from that index as well as its feature vector are entered into a weight update equation which updates the weight if the predicted label and actual label are different. This for loop also keeps track of the score (accuracy) throughout the iterations."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Linearly Separable",
    "text": "Experiment: Linearly Separable\nThe first experiment I ran had to do with linearly separable data, to ensure that the line converges. I created a perceptron object and ran my perceptron.fit function on that object, and the line did end up converging. I printed out my accuracies, and they fluctuated (albeit barely). The accuracy did end up reaching 1.0, proving line convergence. This code (and output) can be observed below.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Not Linearly Separable",
    "text": "Experiment: Not Linearly Separable\nThe second experiment I did had to do with data that was not linearly separable, to see if the line didnâ€™t converge. Indeed, when the data was not linearly separable (as seen below), the line did not converge and the accuracy never reached 1.0. I utilized the make_circles function from sklearn.datasets to create my non linearly separable data, and the accuracy ended up (after max_steps equal to 1000) at 0.53.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_circles(n_samples = 100, noise = 0.5)\n\n#X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.5, 0.5, 0.5, 0.58, 0.59, 0.59, 0.59, 0.59, 0.47, 0.47, 0.47, 0.55, 0.55, 0.5, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.53, 0.53, 0.54, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.61, 0.61, 0.54, 0.54, 0.5, 0.57, 0.57, 0.57, 0.57, 0.53, 0.53, 0.53, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.56, 0.56, 0.56, 0.48, 0.48, 0.56, 0.56, 0.56, 0.5, 0.5, 0.5, 0.57, 0.49, 0.58, 0.58, 0.57, 0.57, 0.57, 0.62, 0.62, 0.62, 0.62, 0.47, 0.47, 0.47, 0.52, 0.45, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.47, 0.47, 0.47, 0.51, 0.51, 0.5, 0.45, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.45, 0.45, 0.55, 0.55, 0.53, 0.49, 0.51, 0.62, 0.62, 0.44, 0.44, 0.5, 0.44, 0.44, 0.44, 0.51, 0.51, 0.46, 0.47, 0.51, 0.51, 0.51, 0.51, 0.58, 0.49, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.54, 0.54, 0.49, 0.49, 0.56, 0.56, 0.56, 0.49, 0.49, 0.49, 0.49, 0.56, 0.55, 0.6, 0.51, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.53, 0.51, 0.51, 0.5, 0.43, 0.43, 0.43, 0.43, 0.51, 0.51, 0.51, 0.53, 0.49, 0.46, 0.46, 0.53, 0.53, 0.53, 0.56, 0.56, 0.52, 0.52, 0.5, 0.54, 0.54, 0.49, 0.59, 0.5, 0.5, 0.5, 0.5, 0.48, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.56, 0.56, 0.52, 0.52, 0.57, 0.57, 0.57, 0.57, 0.49, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.52, 0.47, 0.54, 0.58, 0.58, 0.58, 0.58, 0.58, 0.57, 0.57, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.57, 0.56, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.55, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.51, 0.51, 0.51, 0.51, 0.56, 0.5, 0.5, 0.53, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.52, 0.52, 0.52, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5, 0.5, 0.52, 0.55, 0.56, 0.56, 0.56, 0.56, 0.59, 0.59, 0.5, 0.5, 0.55, 0.5, 0.5, 0.58, 0.58, 0.56, 0.55, 0.55, 0.55, 0.55, 0.47, 0.53, 0.48, 0.46, 0.46, 0.45, 0.49, 0.44, 0.49, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.49, 0.49, 0.49, 0.49, 0.6, 0.6, 0.6, 0.5, 0.55, 0.5, 0.5, 0.55, 0.55, 0.5, 0.5, 0.46, 0.55, 0.55, 0.55, 0.55, 0.55, 0.49, 0.44, 0.48, 0.48, 0.48, 0.48, 0.44, 0.46, 0.43, 0.5, 0.5, 0.5, 0.5, 0.51, 0.45, 0.56, 0.52, 0.52, 0.6, 0.55, 0.55, 0.55, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.51, 0.51, 0.51, 0.58, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.47, 0.47, 0.45, 0.45, 0.45, 0.45, 0.46, 0.46, 0.46, 0.5, 0.5, 0.51, 0.49, 0.48, 0.55, 0.55, 0.51, 0.51, 0.58, 0.58, 0.55, 0.55, 0.55, 0.55, 0.5, 0.53, 0.52, 0.51, 0.51, 0.46, 0.46, 0.46, 0.46, 0.53, 0.53, 0.51, 0.51, 0.5, 0.5, 0.55, 0.5, 0.46, 0.45, 0.45, 0.43, 0.43, 0.46, 0.46, 0.46, 0.5, 0.5, 0.5, 0.5, 0.42, 0.47, 0.47, 0.47, 0.47, 0.51, 0.51, 0.51, 0.51, 0.51, 0.43, 0.54, 0.44, 0.44, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.5, 0.47, 0.46, 0.55, 0.57, 0.57, 0.57, 0.5, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.6, 0.5, 0.46, 0.46, 0.46, 0.46, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.5, 0.5, 0.5, 0.5, 0.51, 0.53, 0.53, 0.53, 0.53, 0.54, 0.54, 0.53, 0.53, 0.46, 0.46, 0.46, 0.46, 0.48, 0.5, 0.44, 0.44, 0.44, 0.51, 0.5, 0.5, 0.44, 0.48, 0.48, 0.46, 0.46, 0.47, 0.5, 0.5, 0.5, 0.49, 0.46, 0.46, 0.53, 0.45, 0.45, 0.44, 0.44, 0.49, 0.42, 0.53, 0.53, 0.47, 0.47, 0.47, 0.47, 0.47, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.6, 0.6, 0.52, 0.52, 0.57, 0.58, 0.56, 0.57, 0.59, 0.59, 0.59, 0.59, 0.59, 0.55, 0.55, 0.55, 0.55, 0.6, 0.47, 0.51, 0.56, 0.56, 0.51, 0.57, 0.55, 0.55, 0.55, 0.55, 0.54, 0.51, 0.56, 0.47, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.55, 0.46, 0.59, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.53, 0.53, 0.53, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.5, 0.5, 0.55, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.48, 0.48, 0.48, 0.48, 0.56, 0.56, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.5, 0.5, 0.49, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.5, 0.58, 0.58, 0.56, 0.56, 0.56, 0.56, 0.56, 0.57, 0.48, 0.48, 0.48, 0.48, 0.49, 0.57, 0.57, 0.57, 0.55, 0.55, 0.53, 0.53, 0.55, 0.55, 0.55, 0.5, 0.5, 0.5, 0.55, 0.55, 0.51, 0.51, 0.51, 0.56, 0.56, 0.54, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.5, 0.54, 0.5, 0.41, 0.52, 0.52, 0.54, 0.54, 0.54, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.46, 0.48, 0.47, 0.47, 0.47, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.46, 0.5, 0.53, 0.53, 0.55, 0.55, 0.55, 0.49, 0.52, 0.52, 0.46, 0.46, 0.46, 0.49, 0.54, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.55, 0.55, 0.55, 0.52, 0.52, 0.51, 0.51, 0.51, 0.49, 0.49, 0.49, 0.49, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.5, 0.5, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.45, 0.49, 0.45, 0.45, 0.41, 0.52, 0.52, 0.42, 0.42, 0.49, 0.49, 0.45, 0.52, 0.45, 0.51, 0.46, 0.46, 0.46, 0.48, 0.48, 0.47, 0.47, 0.5, 0.45, 0.45, 0.46, 0.46, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.47, 0.47, 0.52, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.51, 0.47, 0.47, 0.47, 0.52, 0.52, 0.52, 0.42, 0.5, 0.5, 0.57, 0.57, 0.57, 0.59, 0.49, 0.49, 0.49, 0.49, 0.53, 0.53, 0.53, 0.53, 0.51, 0.45, 0.46, 0.46, 0.46, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.52, 0.6, 0.6, 0.6, 0.54, 0.55, 0.54, 0.54, 0.53, 0.53, 0.51, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.57, 0.57, 0.48, 0.45, 0.42, 0.48, 0.48, 0.48, 0.53, 0.54, 0.54, 0.49, 0.49, 0.49, 0.47, 0.47, 0.5, 0.5, 0.58, 0.56, 0.56, 0.56, 0.5, 0.56, 0.53, 0.56, 0.5, 0.56, 0.56, 0.53, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.48, 0.5, 0.5, 0.5, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.48, 0.48, 0.48, 0.5, 0.5, 0.55, 0.55, 0.55, 0.51, 0.51, 0.5, 0.61, 0.55, 0.52, 0.52, 0.52, 0.52, 0.47, 0.51, 0.54, 0.5, 0.5, 0.5, 0.53]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Runtime Complexity of a Perceptron Update",
    "text": "Runtime Complexity of a Perceptron Update\nThe runtime complexity of a single iteration of the perceptron algorithm update is dependent on the number of data points n, but not on the number of features p (this is specific to my implementation of the algorithm, where I am looking at accuracy / adding the score to a property history for th perceptron). The runtime is going to be O(max_steps * n) where n is the number of data points. the for loop iterates through the specified number of max_steps, but everything within that for loop (other than the calculation of the score) is O(1). The calculation of the score is O(n), because the score predicts a label for each of the data points. Therefore, within the for loop, the runtime is O(n), making the total runtime O(max_steps * n).\nnote for Phil I think something is up with my accuracies, but not sure whatâ€¦ maybe the way Iâ€™m assigning my original weight?"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques youâ€™ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bridget Ulian CS0145 Blog",
    "section": "",
    "text": "About this blog"
  }
]