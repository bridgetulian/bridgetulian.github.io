[
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the logistic regression algorithm hinged on the correct deduction of partial derivatives from the logistic loss function. My source code, source code here, makes use of several different class functions to calculate the gradient descent. I split my logistic loss and empirical risk functions into two separate functions, as well as my predict, sigmoid, and gradient calculations. I found this easiest when bringing all aspects together in the fit function. My fit function, therefore, is not doing all the heavy work. Most of the work is spread throughout several smaller functions. Stochastic gradient descent followed naturally from gradient descent, and implementation of aforementioned small functions helped when developing the stochastic_fit method.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic_regression import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nnp.seterr(all='ignore')\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Freature 2\")\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.3, max_epochs = 1000)\n\nLR.w\n\nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 1: Learning Rate is Too Large",
    "text": "Experiment 1: Learning Rate is Too Large\nWhen the learning rate is too large, there is no convergence on a minimizer (as demonstrated below).\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 2: Batch Size Changes",
    "text": "Experiment 2: Batch Size Changes\nExample 1: Batch size of 600\nA higher batch size leads to a stochastic gradient descent that is much more similar to gradient descent – converging on a minimizer slower than a smaller batch size.\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size = 600)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nExample 2: Batch size of 10\nMuch more different than the gradient descent (which makes sense, seeing as stochastic gradient descent is essentially stochastic gradient descent with batch size of len(data)). Therefore converges quicker to a minimizer.\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size=2)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experimentation Findings",
    "text": "Experimentation Findings"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Learning Rate",
    "text": "Learning Rate\nThere is a balance between finding a learning rate that finds a quick weight array to separate the data and a learning rate that is so large that it freaks out. The large alpha experiment led to no convergence, and a loss graph that demonstrates some sort of ‘freak out’. Finding the correct learning rate is important in a successful logistic regression model."
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Batch Size",
    "text": "Batch Size\nA larger batch size is more similar to gradient descent, meaning that it converges to a minimizer slower than a smaller batch size. This makes sense logically (as mentioned above), because gradient descent is stochastic gradient descent with batch size len(data)."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Here is a link to my source code on the GitHub repo perceptron.py.\nThe perceptron update is done in my perceptron.fit() function, in which I begin by modifying the features matrix X by adding a row of ones to the features matrix to ensure that the bias is taken into account in the update (as w tilde is a vector of the weights and the bias). The function begins by assigning a random weight vector, then entering into a loop as long as the maximum steps. In this loop, a random index is chosen and then the point from that index as well as its feature vector are entered into a weight update equation which updates the weight if the predicted label and actual label are different. This for loop also keeps track of the score (accuracy) throughout the iterations."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Linearly Separable",
    "text": "Experiment: Linearly Separable\nThe first experiment I ran had to do with linearly separable data, to ensure that the line converges. I created a perceptron object and ran my perceptron.fit function on that object, and the line did end up converging. I printed out my accuracies, and they fluctuated (albeit barely). The accuracy did end up reaching 1.0, proving line convergence. This code (and output) can be observed below.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Not Linearly Separable",
    "text": "Experiment: Not Linearly Separable\nThe second experiment I did had to do with data that was not linearly separable, to see if the line didn’t converge. Indeed, when the data was not linearly separable (as seen below), the line did not converge and the accuracy never reached 1.0. I utilized the make_circles function from sklearn.datasets to create my non linearly separable data, and the accuracy ended up (after max_steps equal to 1000) at 0.53.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_circles(n_samples = 100, noise = 0.5)\n\n#X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.5, 0.5, 0.5, 0.58, 0.59, 0.59, 0.59, 0.59, 0.47, 0.47, 0.47, 0.55, 0.55, 0.5, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.53, 0.53, 0.54, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.61, 0.61, 0.54, 0.54, 0.5, 0.57, 0.57, 0.57, 0.57, 0.53, 0.53, 0.53, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.56, 0.56, 0.56, 0.48, 0.48, 0.56, 0.56, 0.56, 0.5, 0.5, 0.5, 0.57, 0.49, 0.58, 0.58, 0.57, 0.57, 0.57, 0.62, 0.62, 0.62, 0.62, 0.47, 0.47, 0.47, 0.52, 0.45, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.47, 0.47, 0.47, 0.51, 0.51, 0.5, 0.45, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.45, 0.45, 0.55, 0.55, 0.53, 0.49, 0.51, 0.62, 0.62, 0.44, 0.44, 0.5, 0.44, 0.44, 0.44, 0.51, 0.51, 0.46, 0.47, 0.51, 0.51, 0.51, 0.51, 0.58, 0.49, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.54, 0.54, 0.49, 0.49, 0.56, 0.56, 0.56, 0.49, 0.49, 0.49, 0.49, 0.56, 0.55, 0.6, 0.51, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.53, 0.51, 0.51, 0.5, 0.43, 0.43, 0.43, 0.43, 0.51, 0.51, 0.51, 0.53, 0.49, 0.46, 0.46, 0.53, 0.53, 0.53, 0.56, 0.56, 0.52, 0.52, 0.5, 0.54, 0.54, 0.49, 0.59, 0.5, 0.5, 0.5, 0.5, 0.48, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.56, 0.56, 0.52, 0.52, 0.57, 0.57, 0.57, 0.57, 0.49, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.52, 0.47, 0.54, 0.58, 0.58, 0.58, 0.58, 0.58, 0.57, 0.57, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.57, 0.56, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.55, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.51, 0.51, 0.51, 0.51, 0.56, 0.5, 0.5, 0.53, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.52, 0.52, 0.52, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5, 0.5, 0.52, 0.55, 0.56, 0.56, 0.56, 0.56, 0.59, 0.59, 0.5, 0.5, 0.55, 0.5, 0.5, 0.58, 0.58, 0.56, 0.55, 0.55, 0.55, 0.55, 0.47, 0.53, 0.48, 0.46, 0.46, 0.45, 0.49, 0.44, 0.49, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.49, 0.49, 0.49, 0.49, 0.6, 0.6, 0.6, 0.5, 0.55, 0.5, 0.5, 0.55, 0.55, 0.5, 0.5, 0.46, 0.55, 0.55, 0.55, 0.55, 0.55, 0.49, 0.44, 0.48, 0.48, 0.48, 0.48, 0.44, 0.46, 0.43, 0.5, 0.5, 0.5, 0.5, 0.51, 0.45, 0.56, 0.52, 0.52, 0.6, 0.55, 0.55, 0.55, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.51, 0.51, 0.51, 0.58, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.47, 0.47, 0.45, 0.45, 0.45, 0.45, 0.46, 0.46, 0.46, 0.5, 0.5, 0.51, 0.49, 0.48, 0.55, 0.55, 0.51, 0.51, 0.58, 0.58, 0.55, 0.55, 0.55, 0.55, 0.5, 0.53, 0.52, 0.51, 0.51, 0.46, 0.46, 0.46, 0.46, 0.53, 0.53, 0.51, 0.51, 0.5, 0.5, 0.55, 0.5, 0.46, 0.45, 0.45, 0.43, 0.43, 0.46, 0.46, 0.46, 0.5, 0.5, 0.5, 0.5, 0.42, 0.47, 0.47, 0.47, 0.47, 0.51, 0.51, 0.51, 0.51, 0.51, 0.43, 0.54, 0.44, 0.44, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.5, 0.47, 0.46, 0.55, 0.57, 0.57, 0.57, 0.5, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.6, 0.5, 0.46, 0.46, 0.46, 0.46, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.5, 0.5, 0.5, 0.5, 0.51, 0.53, 0.53, 0.53, 0.53, 0.54, 0.54, 0.53, 0.53, 0.46, 0.46, 0.46, 0.46, 0.48, 0.5, 0.44, 0.44, 0.44, 0.51, 0.5, 0.5, 0.44, 0.48, 0.48, 0.46, 0.46, 0.47, 0.5, 0.5, 0.5, 0.49, 0.46, 0.46, 0.53, 0.45, 0.45, 0.44, 0.44, 0.49, 0.42, 0.53, 0.53, 0.47, 0.47, 0.47, 0.47, 0.47, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.6, 0.6, 0.52, 0.52, 0.57, 0.58, 0.56, 0.57, 0.59, 0.59, 0.59, 0.59, 0.59, 0.55, 0.55, 0.55, 0.55, 0.6, 0.47, 0.51, 0.56, 0.56, 0.51, 0.57, 0.55, 0.55, 0.55, 0.55, 0.54, 0.51, 0.56, 0.47, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.55, 0.46, 0.59, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.53, 0.53, 0.53, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.5, 0.5, 0.55, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.48, 0.48, 0.48, 0.48, 0.56, 0.56, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.5, 0.5, 0.49, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.5, 0.58, 0.58, 0.56, 0.56, 0.56, 0.56, 0.56, 0.57, 0.48, 0.48, 0.48, 0.48, 0.49, 0.57, 0.57, 0.57, 0.55, 0.55, 0.53, 0.53, 0.55, 0.55, 0.55, 0.5, 0.5, 0.5, 0.55, 0.55, 0.51, 0.51, 0.51, 0.56, 0.56, 0.54, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.5, 0.54, 0.5, 0.41, 0.52, 0.52, 0.54, 0.54, 0.54, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.46, 0.48, 0.47, 0.47, 0.47, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.46, 0.5, 0.53, 0.53, 0.55, 0.55, 0.55, 0.49, 0.52, 0.52, 0.46, 0.46, 0.46, 0.49, 0.54, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.55, 0.55, 0.55, 0.52, 0.52, 0.51, 0.51, 0.51, 0.49, 0.49, 0.49, 0.49, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.5, 0.5, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.45, 0.49, 0.45, 0.45, 0.41, 0.52, 0.52, 0.42, 0.42, 0.49, 0.49, 0.45, 0.52, 0.45, 0.51, 0.46, 0.46, 0.46, 0.48, 0.48, 0.47, 0.47, 0.5, 0.45, 0.45, 0.46, 0.46, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.47, 0.47, 0.52, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.51, 0.47, 0.47, 0.47, 0.52, 0.52, 0.52, 0.42, 0.5, 0.5, 0.57, 0.57, 0.57, 0.59, 0.49, 0.49, 0.49, 0.49, 0.53, 0.53, 0.53, 0.53, 0.51, 0.45, 0.46, 0.46, 0.46, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.52, 0.6, 0.6, 0.6, 0.54, 0.55, 0.54, 0.54, 0.53, 0.53, 0.51, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.57, 0.57, 0.48, 0.45, 0.42, 0.48, 0.48, 0.48, 0.53, 0.54, 0.54, 0.49, 0.49, 0.49, 0.47, 0.47, 0.5, 0.5, 0.58, 0.56, 0.56, 0.56, 0.5, 0.56, 0.53, 0.56, 0.5, 0.56, 0.56, 0.53, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.48, 0.5, 0.5, 0.5, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.48, 0.48, 0.48, 0.5, 0.5, 0.55, 0.55, 0.55, 0.51, 0.51, 0.5, 0.61, 0.55, 0.52, 0.52, 0.52, 0.52, 0.47, 0.51, 0.54, 0.5, 0.5, 0.5, 0.53]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Runtime Complexity of a Perceptron Update",
    "text": "Runtime Complexity of a Perceptron Update\nThe runtime complexity of a single iteration of the perceptron algorithm update is dependent on the number of data points n, but not on the number of features p (this is specific to my implementation of the algorithm, where I am looking at accuracy / adding the score to a property history for th perceptron). The runtime is going to be O(max_steps * n) where n is the number of data points. the for loop iterates through the specified number of max_steps, but everything within that for loop (other than the calculation of the score) is O(1). The calculation of the score is O(n), because the score predicts a label for each of the data points. Therefore, within the for loop, the runtime is O(n), making the total runtime O(max_steps * n).\nnote for Phil I think something is up with my accuracies, but not sure what… maybe the way I’m assigning my original weight?"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "Palmer_Penguins_Blog.html",
    "href": "Palmer_Penguins_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n    \n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.6617647058823529\n\n\n\ndef decision_region_panel(X, y, model, qual_features):  \n  p = len(qual_features)\n  fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n  for i in range(p):\n\n      filler_feature_values = {2+j: 0 for j in range(p)}\n\n      filler_feature_values.update({2+i: 1})\n\n      ix = X[qual_features[i]] == 1\n\n      ax = axarr[i]\n\n      plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n      ax.set_xlabel(X.columns[0])\n      ax.set_ylabel(X.columns[1])\n\n      handles, labels = ax.get_legend_handles_labels()\n      ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"], \n           framealpha=0.3, scatterpoints=1)\n\n  # Adding axes annotations\n  fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n  plt.tight_layout()\n  plt.show()\n    \n    \nqual_features = [\"Clutch Completion_No\", \"Clutch Completion_Yes\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\nto do\none interesting displayed figure with pandas.groupby().aggregate maybe, discussion of figure and table\nmodel: three features of data and model trained on those features that achieves 100% testing accuracy (reproducible process, code up a search) one must be qualitative two must be quantitative\nevaluate: show decision regions of your finished model, split out by qualitative feature"
  },
  {
    "objectID": "Logistic_Regression_Blog.html",
    "href": "Logistic_Regression_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the logistic regression algorithm hinged on the correct deduction of partial derivatives from the logistic loss function. My source code, source code here, makes use of several different class functions to calculate the gradient descent. I split my logistic loss and empirical risk functions into two separate functions, as well as my predict, sigmoid, and gradient calculations. I found this easiest when bringing all aspects together in the fit function. My fit function, therefore, is not doing all the heavy work. Most of the work is spread throughout several smaller functions. Stochastic gradient descent followed naturally from gradient descent, and implementation of aforementioned small functions helped when developing the stochastic_fit method.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic_regression import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nnp.seterr(all='ignore')\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Freature 2\")\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.3, max_epochs = 1000)\n\nLR.w\n\nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "href": "Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 1: Learning Rate is Too Large",
    "text": "Experiment 1: Learning Rate is Too Large\nWhen the learning rate is too large, there is no convergence on a minimizer (as demonstrated below).\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "href": "Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 2: Batch Size Changes",
    "text": "Experiment 2: Batch Size Changes\nExample 1: Batch size of 600\nA higher batch size leads to a stochastic gradient descent that is much more similar to gradient descent – converging on a minimizer slower than a smaller batch size.\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size = 600)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nExample 2: Batch size of 10\nMuch more different than the gradient descent (which makes sense, seeing as stochastic gradient descent is essentially stochastic gradient descent with batch size of len(data)). Therefore converges quicker to a minimizer.\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size=2)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "Logistic_Regression_Blog.html#experimentation-findings",
    "href": "Logistic_Regression_Blog.html#experimentation-findings",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experimentation Findings",
    "text": "Experimentation Findings"
  },
  {
    "objectID": "Logistic_Regression_Blog.html#learning-rate",
    "href": "Logistic_Regression_Blog.html#learning-rate",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Learning Rate",
    "text": "Learning Rate\nThere is a balance between finding a learning rate that finds a quick weight array to separate the data and a learning rate that is so large that it freaks out. The large alpha experiment led to no convergence, and a loss graph that demonstrates some sort of ‘freak out’. Finding the correct learning rate is important in a successful logistic regression model."
  },
  {
    "objectID": "Logistic_Regression_Blog.html#batch-size",
    "href": "Logistic_Regression_Blog.html#batch-size",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Batch Size",
    "text": "Batch Size\nA larger batch size is more similar to gradient descent, meaning that it converges to a minimizer slower than a smaller batch size. This makes sense logically (as mentioned above), because gradient descent is stochastic gradient descent with batch size len(data)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bridget Ulian CS0145 Blog",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Perceptron_Blog.html",
    "href": "Perceptron_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Here is a link to my source code on the GitHub repo perceptron.py.\nThe perceptron update is done in my perceptron.fit() function, in which I begin by modifying the features matrix X by adding a row of ones to the features matrix to ensure that the bias is taken into account in the update (as w tilde is a vector of the weights and the bias). The function begins by assigning a random weight vector, then entering into a loop as long as the maximum steps. In this loop, a random index is chosen and then the point from that index as well as its feature vector are entered into a weight update equation which updates the weight if the predicted label and actual label are different. This for loop also keeps track of the score (accuracy) throughout the iterations."
  },
  {
    "objectID": "Perceptron_Blog.html#experiment-linearly-separable",
    "href": "Perceptron_Blog.html#experiment-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Linearly Separable",
    "text": "Experiment: Linearly Separable\nThe first experiment I ran had to do with linearly separable data, to ensure that the line converges. I created a perceptron object and ran my perceptron.fit function on that object, and the line did end up converging. I printed out my accuracies, and they fluctuated (albeit barely). The accuracy did end up reaching 1.0, proving line convergence. This code (and output) can be observed below.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "Perceptron_Blog.html#experiment-not-linearly-separable",
    "href": "Perceptron_Blog.html#experiment-not-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Not Linearly Separable",
    "text": "Experiment: Not Linearly Separable\nThe second experiment I did had to do with data that was not linearly separable, to see if the line didn’t converge. Indeed, when the data was not linearly separable (as seen below), the line did not converge and the accuracy never reached 1.0. I utilized the make_circles function from sklearn.datasets to create my non linearly separable data, and the accuracy ended up (after max_steps equal to 1000) at 0.53.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_circles(n_samples = 100, noise = 0.5)\n\n#X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.5, 0.5, 0.5, 0.58, 0.59, 0.59, 0.59, 0.59, 0.47, 0.47, 0.47, 0.55, 0.55, 0.5, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.53, 0.53, 0.54, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.61, 0.61, 0.54, 0.54, 0.5, 0.57, 0.57, 0.57, 0.57, 0.53, 0.53, 0.53, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.56, 0.56, 0.56, 0.48, 0.48, 0.56, 0.56, 0.56, 0.5, 0.5, 0.5, 0.57, 0.49, 0.58, 0.58, 0.57, 0.57, 0.57, 0.62, 0.62, 0.62, 0.62, 0.47, 0.47, 0.47, 0.52, 0.45, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.47, 0.47, 0.47, 0.51, 0.51, 0.5, 0.45, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.45, 0.45, 0.55, 0.55, 0.53, 0.49, 0.51, 0.62, 0.62, 0.44, 0.44, 0.5, 0.44, 0.44, 0.44, 0.51, 0.51, 0.46, 0.47, 0.51, 0.51, 0.51, 0.51, 0.58, 0.49, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.54, 0.54, 0.49, 0.49, 0.56, 0.56, 0.56, 0.49, 0.49, 0.49, 0.49, 0.56, 0.55, 0.6, 0.51, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.53, 0.51, 0.51, 0.5, 0.43, 0.43, 0.43, 0.43, 0.51, 0.51, 0.51, 0.53, 0.49, 0.46, 0.46, 0.53, 0.53, 0.53, 0.56, 0.56, 0.52, 0.52, 0.5, 0.54, 0.54, 0.49, 0.59, 0.5, 0.5, 0.5, 0.5, 0.48, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.56, 0.56, 0.52, 0.52, 0.57, 0.57, 0.57, 0.57, 0.49, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.52, 0.47, 0.54, 0.58, 0.58, 0.58, 0.58, 0.58, 0.57, 0.57, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.57, 0.56, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.55, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.51, 0.51, 0.51, 0.51, 0.56, 0.5, 0.5, 0.53, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.52, 0.52, 0.52, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5, 0.5, 0.52, 0.55, 0.56, 0.56, 0.56, 0.56, 0.59, 0.59, 0.5, 0.5, 0.55, 0.5, 0.5, 0.58, 0.58, 0.56, 0.55, 0.55, 0.55, 0.55, 0.47, 0.53, 0.48, 0.46, 0.46, 0.45, 0.49, 0.44, 0.49, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.49, 0.49, 0.49, 0.49, 0.6, 0.6, 0.6, 0.5, 0.55, 0.5, 0.5, 0.55, 0.55, 0.5, 0.5, 0.46, 0.55, 0.55, 0.55, 0.55, 0.55, 0.49, 0.44, 0.48, 0.48, 0.48, 0.48, 0.44, 0.46, 0.43, 0.5, 0.5, 0.5, 0.5, 0.51, 0.45, 0.56, 0.52, 0.52, 0.6, 0.55, 0.55, 0.55, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.51, 0.51, 0.51, 0.58, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.47, 0.47, 0.45, 0.45, 0.45, 0.45, 0.46, 0.46, 0.46, 0.5, 0.5, 0.51, 0.49, 0.48, 0.55, 0.55, 0.51, 0.51, 0.58, 0.58, 0.55, 0.55, 0.55, 0.55, 0.5, 0.53, 0.52, 0.51, 0.51, 0.46, 0.46, 0.46, 0.46, 0.53, 0.53, 0.51, 0.51, 0.5, 0.5, 0.55, 0.5, 0.46, 0.45, 0.45, 0.43, 0.43, 0.46, 0.46, 0.46, 0.5, 0.5, 0.5, 0.5, 0.42, 0.47, 0.47, 0.47, 0.47, 0.51, 0.51, 0.51, 0.51, 0.51, 0.43, 0.54, 0.44, 0.44, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.5, 0.47, 0.46, 0.55, 0.57, 0.57, 0.57, 0.5, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.6, 0.5, 0.46, 0.46, 0.46, 0.46, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.5, 0.5, 0.5, 0.5, 0.51, 0.53, 0.53, 0.53, 0.53, 0.54, 0.54, 0.53, 0.53, 0.46, 0.46, 0.46, 0.46, 0.48, 0.5, 0.44, 0.44, 0.44, 0.51, 0.5, 0.5, 0.44, 0.48, 0.48, 0.46, 0.46, 0.47, 0.5, 0.5, 0.5, 0.49, 0.46, 0.46, 0.53, 0.45, 0.45, 0.44, 0.44, 0.49, 0.42, 0.53, 0.53, 0.47, 0.47, 0.47, 0.47, 0.47, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.6, 0.6, 0.52, 0.52, 0.57, 0.58, 0.56, 0.57, 0.59, 0.59, 0.59, 0.59, 0.59, 0.55, 0.55, 0.55, 0.55, 0.6, 0.47, 0.51, 0.56, 0.56, 0.51, 0.57, 0.55, 0.55, 0.55, 0.55, 0.54, 0.51, 0.56, 0.47, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.55, 0.46, 0.59, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.53, 0.53, 0.53, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.5, 0.5, 0.55, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.48, 0.48, 0.48, 0.48, 0.56, 0.56, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.5, 0.5, 0.49, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.5, 0.58, 0.58, 0.56, 0.56, 0.56, 0.56, 0.56, 0.57, 0.48, 0.48, 0.48, 0.48, 0.49, 0.57, 0.57, 0.57, 0.55, 0.55, 0.53, 0.53, 0.55, 0.55, 0.55, 0.5, 0.5, 0.5, 0.55, 0.55, 0.51, 0.51, 0.51, 0.56, 0.56, 0.54, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.5, 0.54, 0.5, 0.41, 0.52, 0.52, 0.54, 0.54, 0.54, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.46, 0.48, 0.47, 0.47, 0.47, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.46, 0.5, 0.53, 0.53, 0.55, 0.55, 0.55, 0.49, 0.52, 0.52, 0.46, 0.46, 0.46, 0.49, 0.54, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.55, 0.55, 0.55, 0.52, 0.52, 0.51, 0.51, 0.51, 0.49, 0.49, 0.49, 0.49, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.5, 0.5, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.45, 0.49, 0.45, 0.45, 0.41, 0.52, 0.52, 0.42, 0.42, 0.49, 0.49, 0.45, 0.52, 0.45, 0.51, 0.46, 0.46, 0.46, 0.48, 0.48, 0.47, 0.47, 0.5, 0.45, 0.45, 0.46, 0.46, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.47, 0.47, 0.52, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.51, 0.47, 0.47, 0.47, 0.52, 0.52, 0.52, 0.42, 0.5, 0.5, 0.57, 0.57, 0.57, 0.59, 0.49, 0.49, 0.49, 0.49, 0.53, 0.53, 0.53, 0.53, 0.51, 0.45, 0.46, 0.46, 0.46, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.52, 0.6, 0.6, 0.6, 0.54, 0.55, 0.54, 0.54, 0.53, 0.53, 0.51, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.57, 0.57, 0.48, 0.45, 0.42, 0.48, 0.48, 0.48, 0.53, 0.54, 0.54, 0.49, 0.49, 0.49, 0.47, 0.47, 0.5, 0.5, 0.58, 0.56, 0.56, 0.56, 0.5, 0.56, 0.53, 0.56, 0.5, 0.56, 0.56, 0.53, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.48, 0.5, 0.5, 0.5, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.48, 0.48, 0.48, 0.5, 0.5, 0.55, 0.55, 0.55, 0.51, 0.51, 0.5, 0.61, 0.55, 0.52, 0.52, 0.52, 0.52, 0.47, 0.51, 0.54, 0.5, 0.5, 0.5, 0.53]"
  },
  {
    "objectID": "Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "href": "Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Runtime Complexity of a Perceptron Update",
    "text": "Runtime Complexity of a Perceptron Update\nThe runtime complexity of a single iteration of the perceptron algorithm update is dependent on the number of data points n, but not on the number of features p (this is specific to my implementation of the algorithm, where I am looking at accuracy / adding the score to a property history for th perceptron). The runtime is going to be O(max_steps * n) where n is the number of data points. the for loop iterates through the specified number of max_steps, but everything within that for loop (other than the calculation of the score) is O(1). The calculation of the score is O(n), because the score predicts a label for each of the data points. Therefore, within the for loop, the runtime is O(n), making the total runtime O(max_steps * n).\nnote for Phil I think something is up with my accuracies, but not sure what… maybe the way I’m assigning my original weight?"
  }
]