[
  {
    "objectID": "posts/timnit-gebru-blog-post.html",
    "href": "posts/timnit-gebru-blog-post.html",
    "title": "Dr. Timnit Gebru",
    "section": "",
    "text": "In my time at Middlebury College, I have been lucky enough to take two classes that focus on ethics as they pertain to technology: Gender, Technology and Future with Professor Gupta and Politics of Virtual Realities with Professor Stanger. Readings either about or by Dr. Timnit Gebru were included in both classes’ curricula. Dr. Gebru is a computer scientist, an Ethiopian refugee who found political but not emotional asylum in the United States, and someone who has single-handedly pushed research on the ethics of Artificial Intelligence into entirely novel waters.\nDr. Gebru’s current work is focused on the Distributed Artificial Intelligence Research Institute (or DAIR), a collective of multidisciplinary researchers who examine the outcomes of AI technology, particularly as it pertains to the African continent and African immigrants in the United States. She previously acted as Google’s co-lead of the Ethical Artificial Intelligence Team, a position which ended in contention when Google asked Dr. Gebru to not publish a paper examining the dangers of bias in large language models. Dr. Gebru claims she was fired; Google claims she resigned. Either way, Google faced extensive internal and external criticism in response.\nDr. Gebru will be virtually visiting Middlebury College to give a lecture on bias and the social impacts of artificial intelligence, and more narrowly, will be visiting our class for a Q&A on Monday, April 24.\n\n\n\nDr. Gebru’s talk at the conference on Computer Vision and Pattern Recognition 2020 focuses on aspects of bias in artificial intelligence less explored by many discussing the topic. I find her idea of a dominant group close to the money very interesting; it follows a theme I noticed in articles about Dr. Gebru, that she focuses on the power dynamics of AI rather than just the biases. She talks about how visibility isn’t inclusion, which can translate to an understanding that there is bias does not mean those biases disappear.\nIt is easy for companies like Google or Amazon or Microsoft to put out a statement saying “we understand there is bias in our algorithms and datasets. We are working to diversify our datasets and hone our algorithms.” Doing the work is much more difficult and multifaceted. Dr. Gebru explains this very well, particularly in an example of Google attempting to diversify their facial recognition datasets. In doing so, Google put out advertisements asking for darker skinned people to join their dataset in a predatory manner. In a similar way, when developers came to realize gender recognition technology isn’t trained on trans people, they scraped YouTube for images of trans creators without notifying said creators. Furthermore, Dr. Gebru argues this harm towards marginalized people goes even deeper. Why is there a gender recognition system that categorizes based on a binary, socially-constructed idea anyways?\nThe point is it takes a lot of work and time to understand the implications of different technologies. In a competitive, for-profit, industry, work and time are only worth cutting. Why would a company spend the time and resources hiring experts on biases and social implications of a technology when they could make millions of dollars and cut costs simply by sending the product to the public? To harness the power dynamic between for-profit corporations and marginalized individuals, educated experts and resources need to focus on social implications of technology. It is likely that ensuring corporations take the time and resources to hire experts will require government intervention.\ntl;dr Visibility isn’t inclusion, acknowledging biases and inequity in technology development does not solve the problem of said biases and inequity, it takes much more work and depth of research into implications of technology.\n\n\n\nI have a few questions for Dr. Gebru, one which has plagued me since I wrote a paper for the Politics of Virtual Realities and one which is simply a curiosity.\n\nQuote from Meredith Whittaker, the senior advisor on AI to the Federal Trade Commission: “What I am concerned about is the capacity for social control that [AI] gives to a few profit-driven corporations.” Question: Do you think the government has the capacity to regulate the power dynamics between massive for-profit tech corporations and the individual citizen, particularly marginalized citizens? Would this have to be an international institution, or is it feasible for individual governments to have different regulations for tech corporations?\nDo you think if there were an industry-wide oath that all technologists should take, similar to the hippocratic oath, it could help mitigate some of the issues we see in technology? If yes, what would be in that oath?\n\n\n\n\nTo avoid re-enchantment with AI and to retain our human dignity and autonomy, government leaders must take initiative in discussing and questioning how AI fits into our current world; AI gone unchecked will not follow moral or ethical guidelines necessary in decision-making, particularly when it comes to governance. This is not a simple task, particularly at a time where competition is so fierce between the United States and China, two technological and economic superpowers. However, without this discussion, humans at all levels of life will promote AI to the superior thinker in our world. In doing so, humanity will give up its autonomy and dignity. Once artificial intelligence begins making decisions for humans and humans stop questioning the validity or ethical implications of said decisions, regaining human autonomy will be impossible. As Heidegger argues, the questioning of the essence of technology is necessary to avoid becoming a standing-reserve and to continue humanity’s progression. This questioning must start with world leaders, who have the experts and means available to understand the implications of artificial intelligence on us as human beings."
  },
  {
    "objectID": "posts/limits_essay_blog/limits_quantitative_methods_essay_blog.html",
    "href": "posts/limits_essay_blog/limits_quantitative_methods_essay_blog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "References\n\nGroos, Maya, Maeve Wallace, Rachel Hardeman, and Katherine P. Theall. n.d. “Measuring Inequity: A Systematic Review of Methods Used to Quantify Structural Racism.” Journal of Health Disparities Research and Practice 11 (2). https://digitalscholarship.unlv.edu/jhdrp/vol11/iss2/13/ .\n\n\nHardeman, Rachel R., Patricia A. Homan, Tongtan Chantarat, Brigette A. Davis, and Tyson H. Brown. 2022. “Improving the Measurement of Structural Racism to Achieve Antiracist Health Policy.” Health Affairs 41 (2): 179–86. https://doi.org/10.1377/hlthaff.2021.01489.\n\n\nMcCann, Edward, and Michael Brown. 2017. “Discrimination and Resilience and the Needs of People Who Identify as Transgender: A Narrative Review of Quantitative Research Studies.” Journal of Clinical Nursing 26 (23-24): 4080–93. https://doi.org/https://doi.org/10.1111/jocn.13913.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discriminatino.” https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf.\n\n\nRagin, Charles C., Susan E. Mayer, and Kriss A. Drass. 1984. “Assessing Discrimination: A Boolean Approach.” American Sociological Review 49 (2): 221–34. http://www.jstor.org/stable/2095572.\n\n\nVolpe, Vanessa V., Dalal Katsiaficas, Perusi G. Benson, and Susana N. Zelaya Rivera. n.d. “A Mixed Methods Investigation of Black College-Attending Emerging Adults’ Experiences with Multilevel Racism.” American Journal of Orthopsychiatry, 687–702. https://psycnet.apa.org/record/2020-44810-001 ."
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "A link to my source code: source code here.\n\n\nImplementing the logistic regression algorithm hinged on the correct deduction of partial derivatives from the logistic loss function. My source code, source code here, makes use of several different class functions to calculate the gradient descent. I split my logistic loss and empirical risk functions into two separate functions, as well as my predict, sigmoid, and gradient calculations. I found this easiest when bringing all aspects together in the fit function. My fit function, therefore, is not doing all the heavy work. Most of the work is spread throughout several smaller functions. Stochastic gradient descent followed naturally from gradient descent, and implementation of aforementioned small functions helped when developing the stochastic_fit method.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic_regression import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nnp.seterr(all='ignore')\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Freature 2\")\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.3, max_epochs = 1000)\n\nLR.w\n\nfig = draw_line(LR.w, -2, 2)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-1-learning-rate-is-too-large",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 1: Learning Rate is Too Large",
    "text": "Experiment 1: Learning Rate is Too Large\nWhen the learning rate is too large, there is no convergence on a minimizer (as demonstrated below).\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experiment-2-batch-size-changes",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment 2: Batch Size Changes",
    "text": "Experiment 2: Batch Size Changes\nExample 1: Batch size of 600\nA higher batch size leads to a stochastic gradient descent that is much more similar to gradient descent – converging on a minimizer slower than a smaller batch size.\n\n%load_ext autoreload\n%autoreload 2\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size = 600)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nExample 2: Batch size of 10\nMuch more different than the gradient descent (which makes sense, seeing as stochastic gradient descent is essentially stochastic gradient descent with batch size of len(data)). Therefore converges quicker to a minimizer.\n\n%load_ext autoreload\n%autoreload 2\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  alpha = .05,\n                  batch_size=2)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#experimentation-findings",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experimentation Findings",
    "text": "Experimentation Findings"
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#learning-rate",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Learning Rate",
    "text": "Learning Rate\nThere is a balance between finding a learning rate that finds a quick weight array to separate the data and a learning rate that is so large that it freaks out. The large alpha experiment led to no convergence, and a loss graph that demonstrates some sort of ‘freak out’. Finding the correct learning rate is important in a successful logistic regression model."
  },
  {
    "objectID": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "href": "posts/Logistic-Regression-Blog-Post/Logistic_Regression_Blog.html#batch-size",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Batch Size",
    "text": "Batch Size\nA larger batch size is more similar to gradient descent, meaning that it converges to a minimizer slower than a smaller batch size. This makes sense logically (as mentioned above), because gradient descent is stochastic gradient descent with batch size len(data)."
  },
  {
    "objectID": "posts/Palmer_Penguins_Blog.html",
    "href": "posts/Palmer_Penguins_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Throughout this blog post, I determined a model and features which achieves 100% accuracy in the Palmer Penguins Dataset. All code is contained within this notebook.\n\n\nBelow are two displayed figures, one displayed table alongside one displayed figure.\n\n\n\nFrom this aggregation (seen below), it is clear the body mass of male penguins is significantly larger than the mass of female penguins. I hypothesized as much, but was curious how mean body mass’ compared between female and male penguins. On average, male penguins are approximately 800 grams larger than female penguins. This is interesting, not for prediction of species (unless number of females and males is an indicator for species, which is something to explore later), but for prediction of gender.\n\n#one interesting displayed table w/ pandas.groupby().aggregate, discussion of figure/table\n\nX_train.groupby('Sex_FEMALE')['Body Mass (g)'].mean()\n\nSex_FEMALE\n0    4613.076923\n1    3823.214286\nName: Body Mass (g), dtype: float64\n\n\n\n\n\nThe figures below describe the relationship between Culmen Length (mm) and Body Mass (g) in the penguins found in each island. On the left, the relationship for penguins on Dream Island and Togerson Island is shown (orange being Dream Island, blue being Torgerson Island). On the right, the relationship for penguins on Biscoe Island is shown. It is clear that on Biscoe Island there is a pretty linear relationship between increasing Culmen Length and increasing Body Mass. This is curious, as it points to perhaps a more homogenous penguin species population on Biscoe Island – the relationship is consistent for the majority of penguins on the island. Looking at Dream and Torgerson Island on the left, there is almost no relationship between Culmen Length and Body Mass. This could mean a few things; there is a wide variety of penguin species on the two islands, affecting the linearity of the relationship between the two features, or the species of penguin typically found on the two islands does not increase in Body Mass with an increase in Culmen Length.\n\n# one interesting displayed figure with seaborn + discussion\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.lmplot(\ndata=X_train, \nx=\"Culmen Length (mm)\",y=\"Body Mass (g)\", col = \"Island_Biscoe\",\nhue=\"Island_Dream\")\n\n<seaborn.axisgrid.FacetGrid at 0x7ff02872e850>\n\n\n\n\n\n\n\n\nNext, I determined the model and features to use in order to reach 100% accuracy on the testing data. With a reproducible method, I found Logistic Regression and Island, Culmen Length, and Culmen Depth to achieve 100% testing accuracy.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier #max_depth parameter, controls complexity of model, use cross-valudation to find good value of parameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #parameter gamma that controls complexity, use cross-validation to select (cover a wide range of values)\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#how the data works\ntrain.head()\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n#this is just preparing the data i.e. dropping n/as \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 15N', 'Delta 13C']\n\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n     \ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\ncols_to_use = []\nbest_score = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n       # print(cols)\n        # you could train models and score them here, keeping the list of \n        # columns for the model that has the best score. \n        # \n        lr.fit(X_train[cols], y_train)\n        if lr.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"logistic regression\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        dt.fit(X_train[cols], y_train)\n        if dt.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"decision tree\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n         \n        rf.fit(X_train[cols], y_train)\n        if rf.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"random forest\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        svc.fit(X_train[cols], y_train)\n        if svc.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"svc\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n            \n        break\n    \n\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nAgain, via the reproducible model above, I found the Logistic Regression Model and features Island, Culmen Length (mm), and Culmen Depth (mm) should find an accuracy of 100% with the testing data. This is confirmed below, where the model achieves an accuracy of 100% with the test data.\n\nLR = LogisticRegression()\nLR.fit(X_train[cols_to_use], y_train)\nLR.score(X_train[cols_to_use], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.score(X_test[cols_to_use], y_test)\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\n\nAs seen below, utilizing the Logistic Regression Model alongside the features Island, Culmen Length (mm), and Culmen Depth (mm) yields an accuracy of 1.0 and perfectly splits the decision regions.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n    \n    \nplot_regions(LR, X_train[cols_to_use], y_train)"
  },
  {
    "objectID": "posts/Linear-Regression-Blog-Post/linear-regression-blog.html",
    "href": "posts/Linear-Regression-Blog-Post/linear-regression-blog.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Linear regression is a model that helps find the linear equation that best explains the correlation between variables and an outcome. In this blog post, I implement linear regression in two ways.\nThe first is analytical, in which I use the formula: \\[\\hat w = argmin_{w}L(w) = argmin_{w}\\lVert Xw - y \\rVert^2\\] This formula updates the weight according to the loss function: \\[l(\\hat y, y) = (\\hat y - y)^2\\] With this minimization equation, one can then take the gradient with respect to \\(\\hat w\\), and solve for \\(\\hat w\\) as \\[\\hat w = (X^TX)^{-1}X^Ty\\]\nThe second was with gradient descent, the formula for the gfradient being \\[\\nabla L = 2X^T(Xw - y)\\] Each weight update, the gradient multiplied by an alpha value (default alpha = 0.001) was subtracted from the weight to obtain the new weight.\nThese two implementations of linear regression are seen below and my source code can be found at source code.\n\n\n\n\n%load_ext autoreload\n%autoreload 2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nfrom linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.5013\nValidation score = 0.6025\n\n\n\nLR.w \n\narray([0.75348988, 0.27263304])\n\n\n\n\n\nObserve the weight is the same as the analytical linear regression weight.\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train) # I used the analytical formula as my default fit method\n\nLR2.w\n\narray([0.75348774, 0.27263413])\n\n\n\n\n\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\nBelow I ran an experiment; what would happen if I increased the number of features to right below the number of training data points? What occurred, shown very clearly in the graph, is overfitting. As I explain below, the validation score fell far below 0.0 with a significant increase in number of features.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores = []\nval_scores = []\np_features_array = []\n\nwhile p_features <= (n_train - 1):\n    p_features_array.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    training_scores.append(LR.score(X_train, y_train))\n    val_scores.append(LR.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array, val_scores)\nfig = plt.scatter(p_features_array, training_scores)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n# # plot it\n# fig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n# axarr[0].scatter(X_train, y_train)\n# axarr[1].scatter(X_val, y_val)\n# labs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\n# labs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\n# plt.tight_layout()\n\n<matplotlib.legend.Legend at 0x7f9b0167f580>\n\n\n\n\n\nThe above observation of training and validation data demonstrates that increasing the number of features does not always help when it comes to validation data – the validation scores clearly fall off significantly as features expand, although not until the number of features near the value of n_train. This is a clear example of overfitting, when the training scores remain high (nearly 1.0), but the validation scores decrease below 0.0 (exemplary of a very bad model) when the number of features is almost equivalent to the number of data points. Full understanding of the training data does not translate to validation data, but rather handles only the specifics of the training data.\n\n\n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\n\nL.score(X_val, y_val)\n\n0.6076240794781351\n\n\n\n\n\nAs seen below, I demonstrate three graphs which show the validation and training scores for LASSO regularized linear regression models with three different alpha values. In all three, the validation scores decrease, but not nearly with the same drastic change as with regular linear regression.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.001)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n<matplotlib.legend.Legend at 0x7f9b00d88430>\n\n\n\n\n\nWith LASSO regulation, when the number of features gets large the validation scores still drop. However, they remain higher than with normal linear regression. They still fall off significantly when number of feature increase, but they do not fall as far as the linear regression models did (i.e. on this specific data, LASSO falls to a little under 0.5 validation score whereas linear regression falls under 0.0 validation score). This demonstrates how LASSO regularization helps overparametized models – it helps protect the model (at least slightly) from overfitting.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.01)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n<matplotlib.legend.Legend at 0x7f9b018f7280>\n\n\n\n\n\nChanging the alpha (i.e. making the alpha value 0.01 instead of 0.001) changes the validation scores even more significantly. The validation scores fall off much earlier – around p_features = 50 they start decreasing – but they still do not fall as low as the linear regression validation scores (they reach the same score as with alpha = 0.001).\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\n\ntraining_scores2 = []\nval_scores2 = []\np_features_array2 = []\n\nwhile p_features <= (n_train + 1):\n    p_features_array2.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2 = Lasso(alpha = 0.0001)\n    L2.fit(X_train, y_train)\n    training_scores2.append(L2.score(X_train, y_train))\n    val_scores2.append(L2.score(X_val, y_val))\n    p_features += 1\n  \n    \nfig = plt.scatter(p_features_array2, val_scores2)\nfig = plt.scatter(p_features_array2, training_scores2)\nxlab = plt.xlabel(\"Number of Features\")\nylab = plt.ylabel(\"Training and Validation Scores\")\nplt.legend(['Validation Scores', 'Training Scores'])\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e-02, tolerance: 4.368e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e-02, tolerance: 3.745e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e-01, tolerance: 4.979e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.573e-02, tolerance: 4.222e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n<matplotlib.legend.Legend at 0x7f9b01b55a60>\n\n\n\n\n\nOn the other hand, decreasing the alpha from 0.001 to 0.0001 makes the validation scores decrease much slower and to a much lesser extent. In the above graph, the validation scores with Lasso regularization only fall to around 0.75. Decreasing the alpha, therefore, increases the validation scores."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Here is a link to my source code on the GitHub repo perceptron.py.\nThe perceptron update is done in my perceptron.fit() function, in which I begin by modifying the features matrix X by adding a row of ones to the features matrix to ensure that the bias is taken into account in the update (as w tilde is a vector of the weights and the bias). The function begins by assigning a random weight vector, then entering into a loop as long as the maximum steps. In this loop, a random index is chosen and then the point from that index as well as its feature vector are entered into a weight update equation which updates the weight if the predicted label and actual label are different. This for loop also keeps track of the score (accuracy) throughout the iterations."
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Linearly Separable",
    "text": "Experiment: Linearly Separable\nThe first experiment I ran had to do with linearly separable data, to ensure that the line converges. I created a perceptron object and ran my perceptron.fit function on that object, and the line did end up converging. I printed out my accuracies, and they fluctuated (albeit barely). The accuracy did end up reaching 1.0, proving line convergence. This code (and output) can be observed below.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-not-linearly-separable",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Not Linearly Separable",
    "text": "Experiment: Not Linearly Separable\nThe second experiment I did had to do with data that was not linearly separable, to see if the line didn’t converge. Indeed, when the data was not linearly separable (as seen below), the line did not converge and the accuracy never reached 1.0. I utilized the make_circles function from sklearn.datasets to create my non linearly separable data, and the accuracy ended up (after max_steps equal to 1000) at 0.53.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_circles(n_samples = 100, noise = 0.5)\n\n#X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nAccuracy History as an Array\n[0.5, 0.5, 0.5, 0.58, 0.59, 0.59, 0.59, 0.59, 0.47, 0.47, 0.47, 0.55, 0.55, 0.5, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.53, 0.53, 0.54, 0.54, 0.54, 0.54, 0.54, 0.55, 0.55, 0.61, 0.61, 0.54, 0.54, 0.5, 0.57, 0.57, 0.57, 0.57, 0.53, 0.53, 0.53, 0.6, 0.6, 0.6, 0.6, 0.6, 0.55, 0.55, 0.56, 0.56, 0.56, 0.48, 0.48, 0.56, 0.56, 0.56, 0.5, 0.5, 0.5, 0.57, 0.49, 0.58, 0.58, 0.57, 0.57, 0.57, 0.62, 0.62, 0.62, 0.62, 0.47, 0.47, 0.47, 0.52, 0.45, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.47, 0.47, 0.47, 0.51, 0.51, 0.5, 0.45, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.45, 0.45, 0.55, 0.55, 0.53, 0.49, 0.51, 0.62, 0.62, 0.44, 0.44, 0.5, 0.44, 0.44, 0.44, 0.51, 0.51, 0.46, 0.47, 0.51, 0.51, 0.51, 0.51, 0.58, 0.49, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.51, 0.54, 0.54, 0.49, 0.49, 0.56, 0.56, 0.56, 0.49, 0.49, 0.49, 0.49, 0.56, 0.55, 0.6, 0.51, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.53, 0.51, 0.51, 0.5, 0.43, 0.43, 0.43, 0.43, 0.51, 0.51, 0.51, 0.53, 0.49, 0.46, 0.46, 0.53, 0.53, 0.53, 0.56, 0.56, 0.52, 0.52, 0.5, 0.54, 0.54, 0.49, 0.59, 0.5, 0.5, 0.5, 0.5, 0.48, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.56, 0.56, 0.52, 0.52, 0.57, 0.57, 0.57, 0.57, 0.49, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.52, 0.47, 0.54, 0.58, 0.58, 0.58, 0.58, 0.58, 0.57, 0.57, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.57, 0.56, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.55, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.51, 0.51, 0.51, 0.51, 0.56, 0.5, 0.5, 0.53, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.52, 0.52, 0.52, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5, 0.5, 0.52, 0.55, 0.56, 0.56, 0.56, 0.56, 0.59, 0.59, 0.5, 0.5, 0.55, 0.5, 0.5, 0.58, 0.58, 0.56, 0.55, 0.55, 0.55, 0.55, 0.47, 0.53, 0.48, 0.46, 0.46, 0.45, 0.49, 0.44, 0.49, 0.53, 0.53, 0.53, 0.53, 0.53, 0.53, 0.49, 0.49, 0.49, 0.49, 0.6, 0.6, 0.6, 0.5, 0.55, 0.5, 0.5, 0.55, 0.55, 0.5, 0.5, 0.46, 0.55, 0.55, 0.55, 0.55, 0.55, 0.49, 0.44, 0.48, 0.48, 0.48, 0.48, 0.44, 0.46, 0.43, 0.5, 0.5, 0.5, 0.5, 0.51, 0.45, 0.56, 0.52, 0.52, 0.6, 0.55, 0.55, 0.55, 0.5, 0.5, 0.55, 0.55, 0.55, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.51, 0.51, 0.51, 0.58, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.47, 0.47, 0.45, 0.45, 0.45, 0.45, 0.46, 0.46, 0.46, 0.5, 0.5, 0.51, 0.49, 0.48, 0.55, 0.55, 0.51, 0.51, 0.58, 0.58, 0.55, 0.55, 0.55, 0.55, 0.5, 0.53, 0.52, 0.51, 0.51, 0.46, 0.46, 0.46, 0.46, 0.53, 0.53, 0.51, 0.51, 0.5, 0.5, 0.55, 0.5, 0.46, 0.45, 0.45, 0.43, 0.43, 0.46, 0.46, 0.46, 0.5, 0.5, 0.5, 0.5, 0.42, 0.47, 0.47, 0.47, 0.47, 0.51, 0.51, 0.51, 0.51, 0.51, 0.43, 0.54, 0.44, 0.44, 0.49, 0.49, 0.49, 0.51, 0.51, 0.51, 0.5, 0.47, 0.46, 0.55, 0.57, 0.57, 0.57, 0.5, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.6, 0.5, 0.46, 0.46, 0.46, 0.46, 0.45, 0.44, 0.44, 0.44, 0.44, 0.44, 0.5, 0.5, 0.5, 0.5, 0.51, 0.53, 0.53, 0.53, 0.53, 0.54, 0.54, 0.53, 0.53, 0.46, 0.46, 0.46, 0.46, 0.48, 0.5, 0.44, 0.44, 0.44, 0.51, 0.5, 0.5, 0.44, 0.48, 0.48, 0.46, 0.46, 0.47, 0.5, 0.5, 0.5, 0.49, 0.46, 0.46, 0.53, 0.45, 0.45, 0.44, 0.44, 0.49, 0.42, 0.53, 0.53, 0.47, 0.47, 0.47, 0.47, 0.47, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.53, 0.6, 0.6, 0.52, 0.52, 0.57, 0.58, 0.56, 0.57, 0.59, 0.59, 0.59, 0.59, 0.59, 0.55, 0.55, 0.55, 0.55, 0.6, 0.47, 0.51, 0.56, 0.56, 0.51, 0.57, 0.55, 0.55, 0.55, 0.55, 0.54, 0.51, 0.56, 0.47, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.55, 0.46, 0.59, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.55, 0.53, 0.53, 0.53, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.5, 0.5, 0.55, 0.55, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.48, 0.48, 0.48, 0.48, 0.56, 0.56, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.5, 0.5, 0.49, 0.47, 0.5, 0.58, 0.58, 0.58, 0.58, 0.58, 0.58, 0.5, 0.5, 0.5, 0.5, 0.58, 0.58, 0.56, 0.56, 0.56, 0.56, 0.56, 0.57, 0.48, 0.48, 0.48, 0.48, 0.49, 0.57, 0.57, 0.57, 0.55, 0.55, 0.53, 0.53, 0.55, 0.55, 0.55, 0.5, 0.5, 0.5, 0.55, 0.55, 0.51, 0.51, 0.51, 0.56, 0.56, 0.54, 0.54, 0.54, 0.54, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.58, 0.5, 0.54, 0.5, 0.41, 0.52, 0.52, 0.54, 0.54, 0.54, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.48, 0.46, 0.48, 0.47, 0.47, 0.47, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.46, 0.46, 0.5, 0.53, 0.53, 0.55, 0.55, 0.55, 0.49, 0.52, 0.52, 0.46, 0.46, 0.46, 0.49, 0.54, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.55, 0.55, 0.55, 0.52, 0.52, 0.51, 0.51, 0.51, 0.49, 0.49, 0.49, 0.49, 0.52, 0.56, 0.51, 0.56, 0.56, 0.56, 0.5, 0.5, 0.51, 0.51, 0.5, 0.5, 0.5, 0.5, 0.45, 0.49, 0.45, 0.45, 0.41, 0.52, 0.52, 0.42, 0.42, 0.49, 0.49, 0.45, 0.52, 0.45, 0.51, 0.46, 0.46, 0.46, 0.48, 0.48, 0.47, 0.47, 0.5, 0.45, 0.45, 0.46, 0.46, 0.56, 0.56, 0.56, 0.56, 0.56, 0.51, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.47, 0.47, 0.52, 0.47, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.51, 0.47, 0.47, 0.47, 0.52, 0.52, 0.52, 0.42, 0.5, 0.5, 0.57, 0.57, 0.57, 0.59, 0.49, 0.49, 0.49, 0.49, 0.53, 0.53, 0.53, 0.53, 0.51, 0.45, 0.46, 0.46, 0.46, 0.49, 0.56, 0.56, 0.56, 0.56, 0.56, 0.52, 0.6, 0.6, 0.6, 0.54, 0.55, 0.54, 0.54, 0.53, 0.53, 0.51, 0.5, 0.5, 0.52, 0.5, 0.5, 0.5, 0.5, 0.45, 0.5, 0.5, 0.5, 0.5, 0.56, 0.56, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.57, 0.57, 0.48, 0.45, 0.42, 0.48, 0.48, 0.48, 0.53, 0.54, 0.54, 0.49, 0.49, 0.49, 0.47, 0.47, 0.5, 0.5, 0.58, 0.56, 0.56, 0.56, 0.5, 0.56, 0.53, 0.56, 0.5, 0.56, 0.56, 0.53, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.51, 0.48, 0.5, 0.5, 0.5, 0.49, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.57, 0.48, 0.48, 0.48, 0.5, 0.5, 0.55, 0.55, 0.55, 0.51, 0.51, 0.5, 0.61, 0.55, 0.52, 0.52, 0.52, 0.52, 0.47, 0.51, 0.54, 0.5, 0.5, 0.5, 0.53]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-multiple-dimensions",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#experiment-multiple-dimensions",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Experiment: Multiple Dimensions",
    "text": "Experiment: Multiple Dimensions\nThe third and final experiment has to do with the perceptron algorithm working on more than 2 dimensions. I created a data set with 5 features and then printed the score evolution. Even with multiple dimensions, the algorithm converges to an accuracy of 1.0.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nprint(\"Accuracy History as an Array\")\nprint(p.history)\n\nAccuracy History as an Array\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
  },
  {
    "objectID": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "href": "posts/perceptron-blog-post/Perceptron_Blog.html#runtime-complexity-of-a-perceptron-update",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "Runtime Complexity of a Perceptron Update",
    "text": "Runtime Complexity of a Perceptron Update\nThe runtime complexity of a single iteration of the perceptron algorithm update is dependent on the number of data points n, but not on the number of features p (this is specific to my implementation of the algorithm, where I am looking at accuracy / adding the score to a property history for th perceptron). The runtime is going to be O(max_steps * n) where n is the number of data points. the for loop iterates through the specified number of max_steps, but everything within that for loop (other than the calculation of the score) is O(1). The calculation of the score is O(n), because the score predicts a label for each of the data points. Therefore, within the for loop, the runtime is O(n), making the total runtime O(max_steps * n)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "Palmer_Penguins_Blog.html",
    "href": "Palmer_Penguins_Blog.html",
    "title": "Bridget Ulian's CSCI 0451 Blog",
    "section": "",
    "text": "Throughout this blog post, I determined a model and features which achieves 100% accuracy in the Palmer Penguins Dataset. All code is contained within this notebook.\n\n\nBelow are two displayed figures, one displayed table alongside one displayed figure.\n\n\n\nFrom this aggregation (seen below), it is clear the body mass of male penguins is significantly larger than the mass of female penguins. I hypothesized as much, but was curious how mean body mass’ compared between female and male penguins. On average, male penguins are approximately 800 grams larger than female penguins. This is interesting, not for prediction of species (unless number of females and males is an indicator for species, which is something to explore later), but for prediction of gender.\n\n#one interesting displayed table w/ pandas.groupby().aggregate, discussion of figure/table\n\nX_train.groupby('Sex_FEMALE')['Body Mass (g)'].mean()\n\nSex_FEMALE\n0    4613.076923\n1    3823.214286\nName: Body Mass (g), dtype: float64\n\n\n\n\n\nThe figures below describe the relationship between Culmen Length (mm) and Body Mass (g) in the penguins found in each island. On the left, the relationship for penguins on Dream Island and Togerson Island is shown (orange being Dream Island, blue being Torgerson Island). On the right, the relationship for penguins on Biscoe Island is shown. It is clear that on Biscoe Island there is a pretty linear relationship between increasing Culmen Length and increasing Body Mass. This is curious, as it points to perhaps a more homogenous penguin species population on Biscoe Island – the relationship is consistent for the majority of penguins on the island. Looking at Dream and Torgerson Island on the left, there is almost no relationship between Culmen Length and Body Mass. This could mean a few things; there is a wide variety of penguin species on the two islands, affecting the linearity of the relationship between the two features, or the species of penguin typically found on the two islands does not increase in Body Mass with an increase in Culmen Length.\n\n# one interesting displayed figure with seaborn + discussion\n\nimport seaborn as sns\n\nsns.set_theme()\n\nsns.lmplot(\ndata=X_train, \nx=\"Culmen Length (mm)\",y=\"Body Mass (g)\", col = \"Island_Biscoe\",\nhue=\"Island_Dream\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f9c71ed0f10>\n\n\n\n\n\n\n\n\nNext, I determined the model and features to use in order to reach 100% accuracy on the testing data. With a reproducible method, I found Logistic Regression and Island, Culmen Length, and Culmen Depth to achieve 100% testing accuracy.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier #max_depth parameter, controls complexity of model, use cross-valudation to find good value of parameter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #parameter gamma that controls complexity, use cross-validation to select (cover a wide range of values)\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#how the data works\ntrain.head()\n\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n#this is just preparing the data i.e. dropping n/as \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n#X_train\n\n\n#selecting the columns that we want\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 15N', 'Delta 13C']\n\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n     \ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\ncols_to_use = []\nbest_score = 0\n\nfor qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols \n       # print(cols)\n        # you could train models and score them here, keeping the list of \n        # columns for the model that has the best score. \n        # \n        lr.fit(X_train[cols], y_train)\n        if lr.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"logistic regression\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        dt.fit(X_train[cols], y_train)\n        if dt.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"decision tree\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n         \n        rf.fit(X_train[cols], y_train)\n        if rf.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"random forest\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n        \n        svc.fit(X_train[cols], y_train)\n        if svc.score(X_train[cols], y_train) > best_score:\n            cols_to_use = cols\n            model = \"svc\"\n            best_score = lr.score(X_train[cols], y_train)\n            \n            \n        break\n    \n\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nAgain, via the reproducible model above, I found the Logistic Regression Model and features Island, Culmen Length (mm), and Culmen Depth (mm) should find an accuracy of 100% with the testing data. This is confirmed below, where the model achieves an accuracy of 100% with the test data.\n\nLR = LogisticRegression()\nLR.fit(X_train[cols_to_use], y_train)\nLR.score(X_train[cols_to_use], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.score(X_test[cols_to_use], y_test)\n\n\n/Users/bridgetulian/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n1.0\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\n\nAs seen below, utilizing the Logistic Regression Model alongside the features Island, Culmen Length (mm), and Culmen Depth (mm) yields an accuracy of 1.0 and perfectly splits the decision regions.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n    \n    \nplot_regions(LR, X_train[cols_to_use], y_train)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final project on determining urban air pollution by income and racial demographics.\n\n\n\n\n\n\nMay 10, 2023\n\n\nBridget Ulian, Kate Kenny, Mia Tarantola\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning with Singular Value Decomposition and Laplacian Spectral Clustering\n\n\n\n\n\n\nApr 26, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA discussion of Dr. Timnit Gebru and her work.\n\n\n\n\n\n\nApr 25, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing linear regression.\n\n\n\n\n\n\nApr 5, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn essay assessing Narayanan’s statement that current quantitative methods justify the status quo, and that they do more harm than good.\n\n\n\n\n\n\nMar 30, 2023\n\n\nBridget Ulian\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bridget Ulian CS0145 Blog",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html",
    "href": "posts/unsupervised_learning_blog.html",
    "title": "Unsupervised Learning Blog",
    "section": "",
    "text": "In this blog, I look at singular value decomposition as it pertains to images. Singular value decomposition is a matrix factorization that can be used to compress images, decreasing the storage needed to store an image.\nThe singular value decomposition of a real matrix A is:\n\\[ A = U*D*V^T\\]\nIn the case that D is a real, diagonal matrix and real matrices U and V are orthogonal matrices. One can use this singular value decomposition to compress images down to much smaller images by selecting a component we can call ‘k’; by only selecting the first k columns of U, the top k singular values in D, and the first k rows of V, one can approximate the original matrix A by matrix multiplying these three new matrices.\nWith smaller values of k, understandably, the reconstructed matrix will be further from the original matrix. With higher values of k, the reconstructed matrix will be more similar to the original matrix.\nBecause images themselves are matrices, one can apply singular value decomposition to images. The code below demonstrates the process.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n\n\nBelow I created a function called svd_reconstruct that uses singular value decomposition to compress an image as explained above. There is some set-up code prior to that function; a function called compare_images that allows a user to compare the original and reconstructed images, a read_image function that reads in an image from the Internet, and a to_greyscale image (with examples) to convert an image to greyscale.\n\ndef compare_images(A, A_):\n    fig, axarr = plt.subplots(1, 2, figsize = (7,3))\n    \n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n    \n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n    \n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://cdn-prd.content.metamorphosis.com/wp-content/uploads/sites/2/2020/10/shutterstock_1073056451-2.jpg\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAfter that set up, below is the actual implementation of the svd_reconstruct function alongside a brief test of an original image and reconstructed image of four components.\n\ndef svd_reconstruct(img, k):\n    # A = U D V(transpose)\n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\n\ngrey_img_2 = svd_reconstruct(grey_img, 4)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\n\n\nThe purpose of singular value decomposition in compressing images is to decrease storage space an image takes up. The function below, svd_experiment, runs through component values of 5, 15, 25, 35, 45, and 55 and explores what percentage of the original storage each image takes up. I increased the value of k until the naked eye (or at least my naked eye) could not perceive a difference between the original image and the reconstructed image. I confirmed that by using the compare_images function.\nAt 55 components, the reconstructed image only took up 13.8% of the original image’s storage space. This is a significant difference in storage for a minute, invisible-to-the-human-eye change in image structure.\n\ndef svd_experiment(img):\n\n    fig = plt.figure(figsize = (8, 8))\n    columns = 2\n    rows = 3\n    \n    k_arr = [5, 15, 25, 35, 45, 55]\n    \n    ax=[]\n    \n        \n    for i in range(0, columns*rows):\n        new_img = svd_reconstruct(img, k_arr[i])\n        \n        #storage \n        amt_storage = (k_arr[i] * (img.shape[0] + img.shape[1] + 1)) / (img.shape[0] * img.shape[1])\n        \n        ax.append(fig.add_subplot(rows, columns, i+1))\n        ax[-1].set_title(f\"{k_arr[i]} components, % storage = {(round(100 * amt_storage,1))}\")\n        \n       # fig.add_subplot(rows, columns, i)\n        plt.imshow(new_img, cmap=\"Greys\")\n        \n    plt.tight_layout()    \n    plt.show()\n        \nsvd_experiment(grey_img)\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 55))\n\n\n\n\n\n\n\nAfter finishing the svd_reconstruct, it is important to note that rarely do people understand image compression by the value of k. They are more likely to use a compression factor, how much they want the image storage to decrease by. Below I implemented a version of svd_reconstruct in which the user imputs the compression factor (i.e. 5 means the image is compressed to 1/5 the original storage space).\nThis required some extra math within the reconstruct function to determine the value of k for a given compression factor. That code can be seen below.\n\ndef svd_reconstruct_extra(img, comp_factor):\n    \n    #get the new size of the compressed image\n    new_size = (img.shape[0] * img.shape[1]) / comp_factor\n    \n    #get the k value (integer value) from the new size of the compressed image\n    k = int(new_size / (img.shape[0] + img.shape[0] + 1)) \n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\nBelow I tested two different compression factors; one very large (80), and one fairly small (10). It is clear that with a higher compression factor – the image taking up significantly less space than the original – the reconstructed image is far different than the original. With a smaller compression factor, 10, the reconstructed image is nearly the same as the original image.\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 80)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 10)\ncompare_images(grey_img, grey_img_2)"
  },
  {
    "objectID": "posts/timnit-gebru-blog-post.html#section-1-dr.-gebrus-argument",
    "href": "posts/timnit-gebru-blog-post.html#section-1-dr.-gebrus-argument",
    "title": "Dr. Timnit Gebru",
    "section": "Section 1: Dr. Gebru’s Argument",
    "text": "Section 1: Dr. Gebru’s Argument\nDr. Timnit Gebru’s argument about AGI and second-wave eugenics is this; people are fixated on AI as a means to a utopia or an apocalypse, a transhuman experience, and in doing so are not paying attention to the current problems of AGI and the fact that reaching that transhuman experience requires discriminatory choices, abusive labor, and deep wealth disparities. She draws a very clear connection between first-wave eugenics (think sterilization of the disabled and people of color, something that I learned recently in my American Psycho class continued deep into the 1960s) and second-wave eugenics. She paints both first and second wave eugenics as problematic in somewhat similar ways; both define intelligence in ways that play into casual racism and promote certain traits as positive that are typically found in wealthy, well-educated, white people. She also touched on the monopolization of AGI and the “race to the bottom” in creating larger, more generalized, more versatile models. The problem with larger models, Timnit argued, is that they do not feed back into the communities they come out of.\nSecond-wave eugenics are harmful, Dr. Gebru argues, partially because they choose what qualifies as ‘intelligent’ and what traits are desirable in humans going forward to a posthuman existence. One thing I found interesting in particular was how often second-wave eugenicists cite Charles Murray. It was clear that Dr. Gebru did not understand the history between Charles Murray and Middlebury College, but that history being as contentious as it is means this point was especially salient. If second-wave eugenicists are worried about being labeled discriminatory and racist, they should not draw a connection between themselves and a homophobic, racist, supposed scholar.\nAnother issue with the second-wave eugenicists is their treatment of AI as some mystical, magical, future superpower. Treating AI as a path toward either utopia or apocalypse takes away from the fact that AI is developing currently, under discriminatory, rushed, and vastly unfair circumstances. AGI is not a future mystical superpower, but a current ailment. In order to change the problematic foundation of Artificial Intelligence, computer scientists and billionaires should focus on how to fix the problems of today; how can we stop large models and corporations from monopolizing the market? How can we provide less abusive career paths for people labeling datasets, or moderating models, or having their images and artwork used in datasets?\nI agree in general with Dr. Gebru’s argument. I think she brings up very interesting points about posthumanism and the TESCREAL community, and I believe I will bring a lot of the context from this talk with me as I continue reading about the forays of Elon Musk and crew. I also liked that she reached beyond the technicalities of artificial intelligence models, beyond the ‘check that your data is not biased’ and ‘think about who these algorithms affect.’ I think both of these things are wildly important, but I was also searching for something new from Dr. Gebru; she definitely provided something new to chew on. I do wish, however, that she touched a little more on her interactions with legislators and her hopes for government regulation going forward. I came close to asking a question about it, but then another question was asked that provided a vague answer to what she looks for in regulation. As a political science major (as well as computer science), I am so intrigued by tech regulation in governance. I would have loved to hear what a typical talk with a member of congress sounds like for Dr. Gebru; does she explain the technicalities to each legislator? What does the current state of AI regulation look like in the government? I know that Congress was mocked endlessly for their questioning of Mark Zuckerberg; is it even possible for today’s Congress to regulate tech with their lack of expertise?"
  },
  {
    "objectID": "posts/timnit-gebru-blog-post.html#reflection",
    "href": "posts/timnit-gebru-blog-post.html#reflection",
    "title": "Dr. Timnit Gebru",
    "section": "Reflection",
    "text": "Reflection\nI loved Dr. Gebru’s talk and I think I could’ve sat there listening to her for a lot longer. She is so thoughtful in her speech, something that I greatly admire about very smart people. I appreciated her expertise in fields outside of technical computer vision and her willingness to dip into the theories behind TESCREAL. This past summer, I found myself searching endlessly for jobs that combine political science/policy and computer science. I stumbled across Schmidt Futures, Eric Schmidt’s philanthropic fellowship for computer science students, applied, and was promptly rejected. I have considered going to graduate school for tech public policy, and am very interested in trying to limit the reach of massive tech corporations. Side note, going to the former CEO of Google’s philanthropic fellowship was probably not the best way to take down big tech corporations. Hearing someone speak who is so knowledgeable in a field I have been trying to find my way into was magical. Dr. Gebru inspired the same fascination in me in just one hour that semester-long classes have inspired.\nLike I said, my search for a job at the crux of political science and computer science was rather difficult. I found myself locked into a corporate software engineering job, with hopes of going to graduate school in the future. Whenever I find the spark waning (money is a very strong pull, losing money a pretty strong push), I feel as though I can think back on Dr. Gebru’s talk. I would do pretty much anything to sit there and pick her brain for hours. Thank you Phil for emailing her to come talk to us."
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#laplacian-spectral-clustering",
    "href": "posts/unsupervised_learning_blog.html#laplacian-spectral-clustering",
    "title": "Unsupervised Learning Blog",
    "section": "Laplacian Spectral Clustering",
    "text": "Laplacian Spectral Clustering\nLaplacian Spectral Clustering, which is demonstrated below, is a clustering algorithm that works whether data is linearly separable or not. This makes it helpful in separating data that is not linearly separable, potentially like graphs or circular divisions.\nLaplacian clustering works in the following manner. First, we get the adjacency matrix of the graph. The adjacency matrix demonstrates the edges between nodes. Let’s call this adjacency matrix A. In order to determine relationships between nodes, we can consider the edges between different clusters; we want few edges that connect nodes from different clusters. Let’s call these edges connecting between edges cuts. We want to minimize the cuts between clusters.\nWe are trying to find a vector z such that the number of cuts are minimized. Finding a binary vector \\(z \\in {0,1}\\) is an NP-hard problem, but we can approximate the binary vector z by doing the following.\nA normalized Laplacian matrix can be defined as the matrix \\[ L = D^{-1}[D - A] \\]\nThe diagonal matrix D is the diagonal matrix where\n\\[  D = \\begin{bmatrix}{\\sum_{i = 1}^{n} a_{i1}} && \\\\ & \\ddots & \\\n\\ && \\sum_{i = 1}^{n} a_{in} \\end{bmatrix} \\]\nWhen we have the normalized Laplacian L, z should be the eigenvector with the second-smallest eigenvalue of the normalized Laplacian. With this binary vector, we can split the non-linear data into clusters.\nI will demonstrate this below, but begin with a little insight into the graph used in this process.\n\nThe Graph\nBelow is the graph nicknamed the “Karate Club Graph”, a graph in which each node is an individual member of a karate club. The edges are measures of social ties.\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nThe karate club ended up splitting between two officers, who we will call Mr. Hi and Officer. Knowing which club each member has gone to, we can create the graph below. It colors each node orange if that member belongs to the Officer club and blue if they belong to the Mr. Hi club.\n\nclubs = nx.get_node_attributes(G, \"club\")\n\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\nLaplacian Spectral Clustering on the Karate Club Graph\nBelow is the process for applying the Laplacian clustering method described above on the karate club graph. The implemented function ‘spectral_clustering’ returns a binary vector, z, that determines which cluster each node belongs to.\n\nfrom sklearn.neighbors import NearestNeighbors\nimport networkx as nx\n\ndef spectral_clustering(G):\n\n    A = nx.adjacency_matrix(G).toarray()\n    \n    #find D, start by getting the sums of each row of A\n    row_sums = np.sum(A, axis = 1)\n    \n    #put sums into diagonal matrix D\n    D = np.diag(row_sums)\n    \n    #find normalized Laplacian \n    L = np.linalg.inv(D)@(D - A)\n    \n    # find z, eigenvector with second smallest eigenvalue of Laplacian\n    eig_vals, eig_vecs = np.linalg.eig(L)\n\n    #sort eigenvalues, get the second smallest eigenvalue\n    vals_sorted = np.sort(eig_vals)\n    z_val = vals_sorted[1]\n    \n    #get index of second smallest eigenvalue and get corresponding eigenvector: this is z\n    ind = eig_vals.tolist().index(z_val)\n    z = eig_vecs[:,ind]\n\n    #make z binary\n    z = np.where(z > 0, 1, 0)\n    return z\n\n\n\nThe Accuracy of Laplacian Clustering\nThe Laplacian clustering did a remarkably good job of separating the nodes into their two clusters or clubs. There was one mistake, node number 8, who went with club Mr. Hi but which my Laplacian clustering algorithm placed with the Officer club. Every other node was placed in the correct cluster. It is clear that Laplacian clustering worked very well in this case.\n\nz = spectral_clustering(G)\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if z[i] == 0 else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        )"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#introduction",
    "href": "posts/unsupervised_learning_blog.html#introduction",
    "title": "Unsupervised Learning Blog",
    "section": "Introduction",
    "text": "Introduction\nIn this blog, I look at singular value decomposition as it pertains to images. Singular value decomposition is a matrix factorization that can be used to compress images, decreasing the storage needed to store an image.\nThe singular value decomposition of a real matrix A is:\n\\[ A = U*D*V^T\\]\nIn the case that D is a real, diagonal matrix and real matrices U and V are orthogonal matrices. One can use this singular value decomposition to compress images down to much smaller images by selecting a component we can call ‘k’; by only selecting the first k columns of U, the top k singular values in D, and the first k rows of V, one can approximate the original matrix A by matrix multiplying these three new matrices.\nWith smaller values of k, understandably, the reconstructed matrix will be further from the original matrix. With higher values of k, the reconstructed matrix will be more similar to the original matrix.\nBecause images themselves are matrices, one can apply singular value decomposition to images. The code below demonstrates the process.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#part-1-image-compression-with-singular-value-decompression",
    "href": "posts/unsupervised_learning_blog.html#part-1-image-compression-with-singular-value-decompression",
    "title": "Unsupervised Learning Blog",
    "section": "Part 1: Image Compression with Singular Value Decompression",
    "text": "Part 1: Image Compression with Singular Value Decompression\nBelow I created a function called svd_reconstruct that uses singular value decomposition to compress an image as explained above. There is some set-up code prior to that function; a function called compare_images that allows a user to compare the original and reconstructed images, a read_image function that reads in an image from the Internet, and a to_greyscale image (with examples) to convert an image to greyscale.\n\ndef compare_images(A, A_):\n    fig, axarr = plt.subplots(1, 2, figsize = (7,3))\n    \n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n    \n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n    \n\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://cdn-prd.content.metamorphosis.com/wp-content/uploads/sites/2/2020/10/shutterstock_1073056451-2.jpg\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAfter that set up, below is the actual implementation of the svd_reconstruct function alongside a brief test of an original image and reconstructed image of four components.\n\ndef svd_reconstruct(img, k):\n    # A = U D V(transpose)\n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\n\ngrey_img_2 = svd_reconstruct(grey_img, 4)\ncompare_images(grey_img, grey_img_2)"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#experimentation",
    "href": "posts/unsupervised_learning_blog.html#experimentation",
    "title": "Unsupervised Learning Blog",
    "section": "Experimentation",
    "text": "Experimentation\nThe purpose of singular value decomposition in compressing images is to decrease storage space an image takes up. The function below, svd_experiment, runs through component values of 5, 15, 25, 35, 45, and 55 and explores what percentage of the original storage each image takes up. I increased the value of k until the naked eye (or at least my naked eye) could not perceive a difference between the original image and the reconstructed image. I confirmed that by using the compare_images function.\nAt 55 components, the reconstructed image only took up 13.8% of the original image’s storage space. This is a significant difference in storage for a minute, invisible-to-the-human-eye change in image structure.\n\ndef svd_experiment(img):\n\n    fig = plt.figure(figsize = (8, 8))\n    columns = 2\n    rows = 3\n    \n    k_arr = [5, 15, 25, 35, 45, 55]\n    \n    ax=[]\n    \n        \n    for i in range(0, columns*rows):\n        new_img = svd_reconstruct(img, k_arr[i])\n        \n        #storage \n        amt_storage = (k_arr[i] * (img.shape[0] + img.shape[1] + 1)) / (img.shape[0] * img.shape[1])\n        \n        ax.append(fig.add_subplot(rows, columns, i+1))\n        ax[-1].set_title(f\"{k_arr[i]} components, % storage = {(round(100 * amt_storage,1))}\")\n        \n       # fig.add_subplot(rows, columns, i)\n        plt.imshow(new_img, cmap=\"Greys\")\n        \n    plt.tight_layout()    \n    plt.show()\n        \nsvd_experiment(grey_img)\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 55))"
  },
  {
    "objectID": "posts/unsupervised_learning_blog.html#extras-modifying-svd_reconstruct-with-compression-factors",
    "href": "posts/unsupervised_learning_blog.html#extras-modifying-svd_reconstruct-with-compression-factors",
    "title": "Unsupervised Learning Blog",
    "section": "Extras: Modifying svd_reconstruct with Compression Factors",
    "text": "Extras: Modifying svd_reconstruct with Compression Factors\nAfter finishing the svd_reconstruct, it is important to note that rarely do people understand image compression by the value of k. They are more likely to use a compression factor, how much they want the image storage to decrease by. Below I implemented a version of svd_reconstruct in which the user imputs the compression factor (i.e. 5 means the image is compressed to 1/5 the original storage space).\nThis required some extra math within the reconstruct function to determine the value of k for a given compression factor. That code can be seen below.\n\ndef svd_reconstruct_extra(img, comp_factor):\n    \n    #get the new size of the compressed image\n    new_size = (img.shape[0] * img.shape[1]) / comp_factor\n    \n    #get the k value (integer value) from the new size of the compressed image\n    k = int(new_size / (img.shape[0] + img.shape[0] + 1)) \n    \n    U, sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    return U_ @ D_ @ V_\n\nBelow I tested two different compression factors; one very large (80), and one fairly small (10). It is clear that with a higher compression factor – the image taking up significantly less space than the original – the reconstructed image is far different than the original. With a smaller compression factor, 10, the reconstructed image is nearly the same as the original image.\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 80)\ncompare_images(grey_img, grey_img_2)\n\n\n\n\n\ngrey_img_2 = svd_reconstruct_extra(grey_img, 10)\ncompare_images(grey_img, grey_img_2)"
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html",
    "href": "posts/final_project_blog/final-project-blog-post.html",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "",
    "text": "Code\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#abstract",
    "href": "posts/final_project_blog/final-project-blog-post.html#abstract",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Abstract",
    "text": "Abstract\nThroughout this project, we addressed the problem of urban air pollution and whether it is related to racial and income demographics. We used multiple machine learning models, such as Logistic Regression, Decision Trees, Support Vector Machines, and others to see if it is possible to predict air quality based upon racial and income demographics from the U.S. Census. Our measure of air quality was the Environmental Protection Agency’s Air Quality Index. We determined feature importance to begin with, then determined which model was most accurate on the testing data. We found that it is possible to predict air quality with up to 93% accuracy on testing data based on racial and income demographics, but that many racial and income demographics were not stastically significant in determining the Air Quality Index. Our source code can be found here."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#introduction",
    "href": "posts/final_project_blog/final-project-blog-post.html#introduction",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Introduction",
    "text": "Introduction\nWe aimed to address the possibility of disparities in urban air pollution based on racial and income demographics from the U.S. Census. This is an important problem to discuss and explore because in the case that disparities in urban air pollution are found based upon racial and income demographics, the solution could help determine where resources for decreasing air pollution should go. There have been a few studies that discuss this issue, including one by Environmental Health Perspectives, that quantifies exposure disparities by race/ethnicity and income in the contiguous United States for the six air pollutants that make up the total Air Quality Index (AQI) (Liu et al. 2021). In the study the researchers found that for each pollutant, the racial or ethnic group with the highest national average exposure was a racial or ethnic minority group. Another, from the American Lung Association discusses the impact of air pollution on premature death, finding that those who live in predominately Black communities are at greater risk of premature death from particle pollution (Association 2023). A study from the Harvard School of Public Health looked at how fine particulate air pollution, otherwise known as PM2.5, affects minority racial groups and low-income populations at higher levels than white populations and higher-income populations (Jbaily, Zhou, and Liu 2022). Another study by Columbia and published in the Environmental Health Perspectives journal also looked at PM2.5 and how nonlinear and linear models show the same direction of association between racial/ethnic demographics and PM2.5 levels (Daouda et al. 2022). A final study we looked at discussed how redlining maps drawn in the 1930s affect air pollution levels (NO2 levels) today, finding that redlining continues to shape systemic environmental exposure disparities in the U.S. today (Lane et al. 2022).\nWe are looking at slightly different information, looking at both linear and nonlinear models and how they relate to the AQI which takes into account the six air pollutants that the first study discusses, put together into a single index used by the Environmental Protection Agency (EPA). We are taking both racial/ethnic demographic information and income information into account. We are looking at urban air pollution specifically. We want to determine how urban demographics affect the AQI of certain areas to limit the scope of our experiment and see how far racial and ethnic demographics reach in their impact on air pollution. We hope that our research helps state, federal, and local governments determine how to allocate resources towards decreasing air pollution and increasing sustainable choices."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#values-statement",
    "href": "posts/final_project_blog/final-project-blog-post.html#values-statement",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Values Statement",
    "text": "Values Statement\nPotential users of our project include government officials, charity groups, lobbyists, citizens in highly polluted or non-polluted areas, or real estate investors. These groups would use our project in significantly different ways. When it comes to government officials, charity groups, and lobbyists, the hope is that decision-makers in power will view disparities in air pollution levels as problematic and worthy of attention. When they observe disparities based on racial or income demographics, hopefully it informs government officials and other decision-makers on where to allocate resources. Promoting sustainable energy, strict enforcement of the Clean AirAact, and other environmental practices in areas unfairly burdened with high air pollution could be a positive outcome of our project.\nHowever, our project could also potentially cause harm. When thinking about real estate investments, poor air quality in lower income neighborhoods could provide more reason to avoid putting money into these areas. Families looking to move, buy, or rent in new neighborhoods may avoid neighborhoods that have poor air quality; if these neighborhoods are already low-income and mainly inhabited by racial and ethnic minorities, it will perpetuate systemic oppression that these communities already face. This could also be a reason for citizens inhabiting areas with better air quality – potentially more likely to be higher income, more white areas – to remain in their neighborhoods and continue investing in their own neighborhoods. This could perpetuate the differences in air quality between lower income and higher income neighborhoods.\nOur personal reasons for working on the project include interest in the EPA data on air pollutants as well as a desire to see if bias plays into air pollution data in American urban centers. I personally began reading news articles and studies covering the effect of income and racial demographics on pollution starting in the middle of high school. Making our own datasets, models, and maps helped us grasp those disparities much more concretely and explore how far these disparities extend. Finally, there is a hope that our data helps understand whether disparities in air pollution, based on the Air Quality Index, exist dependent on racial/ethnic and class demographics. While it is not necessarily right that numbers, data, and statistics are often required to make policy decisions, it is the current way of life. Decision-makers can hopefully utilize projects like ours to make decisions that help allocate resources in a way that decreases disparities.\nBased on this reflection, the world would hopefully be a more equitable, just, joyful, peaceful, and sustainable place based on our technology. With resources allocated towards neighborhoods unfairly suffering from poor air quality due to systemic racism and classism, the United States can hopefully take a step towards paying reparations for the historical and current oppression that has compounded on minority – and particularly low-income minority – groups."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#materials-and-methods",
    "href": "posts/final_project_blog/final-project-blog-post.html#materials-and-methods",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nData Cleaning\nFor this project, we used 3 different data sets: - Air pollution data by county - Income data by county - Racial demographic data by county\nOur first step in the process was data cleaning. We hoped to combine these three data sets into one larger dataset that included air pollution, income, and racial data for each county. We started with the pollution data and dropped any columns that were used for labeling units. (ie. NO2 units: ppm, 03 units, etc). These columns had the same value for all entries and add no additional value. We also dropped the values that are used to directly calculate the AQI, such as ‘NO2 Mean’, as these features would clearly have an impact on AQI beyond racial or income demographics. The environmental data was collected nearly everyday for many years, so we decided to find the average values for each year (per county). We iterated through all of the entries, keeping only the first four character of the date local column (the year). Then we grouped by state, county and date local to get the yearly averages for each county.\nWe determined a binary AQI label based upon suggestions from the Environmental Protection Agency (EPA). The EPA determines ‘good’ and ‘moderate’ air quality as any AQI from 0-100. Beyond that, any AQI above 100 is considered unhealthy for sensitive groups. That information can be found more in depth here. We labeled ‘good’ AQI as 1 and ‘bad’ AQI as 0.\nNext, we incorperated the county income data. This data set only included the fips code, state abbreviation and median household income, so we could left join on fips code and state abbreviation to combine the two data sets.\nThe last step was to incorporate the racial demographic data. This data set had many columns, but to simplify it, we only considered the individual race columns. Again, we inner joined, to only included counties that appeard in both data sets.\nNo we have one data set with state, county, pollution, income and racial demographic data.\n\n\nFeature Importance\nOne of our first steps in methodology was determining feature importance of each of our features. When determining feature importance, or the effect feature has on the predictive power of the model, we used the built in feature importance of the Random Forest model. This feature importance is calculated by the Mean Decrease in Impurity, or the MDI. For each feature, the random forest algorithm calculates the average decrease in impurity across all decision trees while constructing them. In simple terms, it calculates each feature importance as the sum over the number of splits – in all decision trees – that include the future – proportionally to the number of samples that feature splits. Features which increase the purity of each split (or the likelihood that the particular split will lead towards an outcome of 1 or 0) are tagged as having higher feature importances.\nThe bar plots of feature importance can be found below:\n\n\nSomething we noted when documenting feature importances was that all features had relatively low importances. Furthermore, some of the features with highest predictive power, like total population and date local, had little to do with racial and income demographics. We will discuss this more later in the results section.\n\n\nPrediction Models\nWe looked into a few different scikit models and compared the results in order to determine which models we wanted to use in final processes. When looking at models, we kept in mind that the base rate of our dataset is 86.05%. Any accuracies we determined from models should be higher than that base rate.\n\nLogistic Regression\nThe logistic regression model resulted in a training accuracy of 89.2% and a testing accuracy of 86.05%. The testing accuracy was not higher than the base rate, meaning the logistic regression model was not anble to aid in predicting AQI outcomes.\n\n\nLinear Regression\nWe looked at the linear regression model in order test the statistical significance of our different feature variables. The training accuracy was 22.2% and the testing accuracy was 24.7%. We used a p-value of <0.05 a threshold to test for coefficient significance. The table of p-values can be found below.\n\nWhat this table shows is a few features carrying statistical significance in our model. The features that are statistically significant are Date Local, Total Population, and Total Female Percentage. One feature, Asian Alone (M) %, is close to statistical significance at p value = 0.060, but not below our threshold.\nWe will discuss the importance of p values in determining analysis more in the results section.\n\n\nSVM\nTo determine the best gamma value for our SVC model, we utlized cross validation scores. We tested gamma values within the range of \\({6^{-4}}\\) to \\({6^{4}}\\). Our experiment showed that the best gamma value was 0.027777, which resulted in a training accuracy of 93.6% and a testing accuracy of 90.5%. This testing accuracy is above our base rate by 4%.\n\n\nDecision Tree\nCross validation was used to determine the optimal maximum depth for the decision tree model. The optimal maximum depth was 5, the training accuracy was 93.4% and the testing accuracy was 90.07%. This testing accuracy was higher than our base rate.\n\n\n\nVisualization\nWe then plotted some choropleth maps to compare our predicted data to the actual data. We utilized the GeoPandas package in python in order to map our data. We utilized this dataset, United States Map of Counties, to combine a map of the United States counties with racial/income demographics as well as AQI numbers."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#results",
    "href": "posts/final_project_blog/final-project-blog-post.html#results",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Results",
    "text": "Results\n\nInitial Results: Testing Real-Life Biases\nOne of our first results had little to do with modeling and more to do with visualizing the bias from features that would represent bias and showed some importance in our feature importance exploration. The first was household income, which represented the median household income from each county. The initial trend is below.\n\nThis image demonstrates an interesting trend; it is most definitely not linear, but there are clear trendlines. As median household income increases past 80,000 dollars, AQI levels above 70 are nonexistent. It is true that there is no clear ‘as median household income increases, AQI decreases by this amount’ linear relationship, but it is also clear that higher AQIs are associated with lower median household incomes.\nThe second feature that we wanted to explore was the total AQI vs. the percentage of white residents by county. What we found was fairly interesting – as the percentage of white residents increased, the AQI became less stabilized and included much higher AQIs. That trend can be observed below.\n\nHowever, something else to note is that our sample size for higher percentages of white residents was much greater than the sample size for lower percentages of white residents. As percentage of white residents increases, the AQI range increases. However, higher percentages of white residents also increases the likelihood that AQI can be significantly lower (for the most part). With the exception of one remarkably low AQI at a county with ~40 percent white residents, the hlowest AQIs occur in counties with 80 percent or more white residents.\n\n\nSVC Model Results: Accuracy and Auditing\nWe decided to use our SVC model for further analysis. The overall accuracy of our model was 90.5%, but we also looked at the positive predictive value. That is the likelihood that a county predicted to have a low risk AQI actually has a low AQI. Our PPV was 91.1%.\nWe also looked into the overall FPR and FNR, which were 59.3% and 1.4%.\nAt the begining of the project, one of our goals was to investigate the bias of our model, so we decided to audit for income and racial bias.\nWe split our income data into two different counties: counties where the median household income was above $73,125 and those which were under.\nThe accuracy of our model for high income counties was 96.5%, while the accuracy for low income counties was 86.4%. This suggests that our model might have income bias. So, we tested for calibration. Calibration means that the fraction of predicted counties to have a AQI binary score of 1 (low risk) who actually had a score of 1 is the same across all income groups. So we will calculate this metric for both income groups. The proportion of high income counties predicted to have a low risk AQI is 0.50, but the proportion of low income counties predicted to have a low risk AQI was 0.39.\nA model satisfies error rate balance if the false positive and false negative rates are equal across groups. Looking at the previously calculated FPR anf FNR, I would say that this model does not satisfy error rate balance. The FPR for high income counties is 50%, while the FPR for low income counties is 61.22%. The difference between the two rates is 10.1%. In general, the high FPR may stem from a lack of positive observation in the training data itself.\nA model satisfies statistical parity if the proportion of counties classified as having a low risk AQI is the same for each group. So we compare the total number of predicted positives. The proportion of low income counties classifies as having a low risk AQI is 0.196, while the proportion of high income counties classified as having a low risk AQI is 0.058. This model does not satisfy statistical parity. The proportion of counties predicted to have a low risk AQI is not the same for low and high income areas. The is a 14% difference between the two groups.\nWe also decided to investigate the racial biases of the model. We split the data into two different categories. Counties where the majority of the population is white and those where there is not a majoroty white population. The accuracy for counties with a white majority population is 0.9122, but the accuracy for counties without a white majority population is 0.8511. The FPR for counties with a majority white population = 0.58; FNR for counties with a majority white population = 0.0123; FPR for counties without a majority white population = 0.6122; FNR for counties without a majority white population = 0.0199. Ideally a model would have the same vlaue for each metric across all groups, but this is not the case.\nNext, we will look at calibration. The percentage of counties without a majority white population predicted have a low risk AQI who actually had a low risk AQI is 33.33%. The percentage of counties with a majority white population predicted have a low risk AQI who actually had a low risk AQI is 42.0%. This model is not calibrated as there is a 9% difference between the two groups.\nA model satisfies error rate balance if the false positive and false negative rates are equal across groups. Looking at the previously calculated FPR anf FNR, I would say that this model does not satisfy error rate balance. The FPR for counties with a majority white population is 58%, while the FPR for counties without a majority white population is 61%.\nA model satisifes statistical parity if the proportion of counties classified as having a low risk AQI is the same for each group. So we compare the total number of predicted positives. The proportion of counties with a majority white population classified as having a low risk AQI is 0.133 The proportion of counties without a majority white population classified as having a low risk AQI is 0.191. These values are more similiar in comparison to some of the other metrics investigated, but there is still a discrepancy between the two groups.\n\n\nVisualizations\n\n\nCode\nfrom geomapping import Mapping\nimport pandas as pd\nfrom dataCleaning import DatasetClass\nfrom sklearn.svm import SVC\nfrom copy import deepcopy\n\nds = DatasetClass()\n\ndataset = pd.read_csv(\"pollution_income_race.csv\")\n\nX_train, X_test, y_train, y_test = ds.train_test_data(dataset)\n\ndf = pd.read_csv(\"pollution_income_race.csv\")\ndf = df.dropna()\nmp = Mapping()\n\nsvc_model = SVC(gamma = .027777777777777776)\nsvc_model.fit(X_train, y_train)\n\ntest_combined = deepcopy(X_test)\n\ntest_combined[\"Predicted AQI Binary\"] = svc_model.predict(X_test)\ntest_combined[\"Actual AQI Binary\"] = y_test\n\n#plotting predictions\nmp.plot_df(test_combined, \"Predicted AQI Binary\")\n\n\n\n\n\n\n\nCode\nmp = Mapping()\nmp.plot_df(test_combined,\"Actual AQI Binary\")\n\n\n\n\n\nThe above mappings of the actual AQI binary and the predicted AQI binary using our SVC model demonstrates the high accuracy of our model, but also demonstrates the high numbers of positive samples we have in our dataset. One thing that we found when exploring the false positive and negative rates of our model was a disproportionately high false positive rate and a disproportionately low false negative rate. When determining how this occurred, we recognized that our dataset has many more ‘positive’ samples than ‘negative’ samples; in other words, we have a dataset that contains many more counties with ‘good’ air quality index than those with ‘bad’ air quality index. This is not something that is disheartening to learn, but rather carries a positive connotation. If most of the areas we are observing have an EPA-determined ‘good’ air quality index, it means that American urban counties are not all suffering from unhealthy air quality.\nHowever, our model is still able to predict outcomes of ‘good’ or ‘bad’ air quality at a higher accuracy than the base rate. Going back to the statistical significance of our features, the most statistically significant features are the local date, the total population, and the total percentage of females. While this is not necessarily what we were expecting, it still provides interesting. Particularly when it comes to the demographic features, total population having a positive feature importance is very interesting. On one hand, more people utilizing vehicles, breathing out CO2, and otherwise creating pollutants is likely to increase the AQI and decrease air quality. On the other hand, people who are able to afford single-family homes and more room for their families are likely to live in areas with lower total populations. Both of these could help explain the importance of total population in determining air quality. If this project could extend into the future, an interesting project would be examining counties with good and bad air quality side by side and observing how their total populations and living conditions compare.\nOverall, we found that racial/ethnic and income demographics hold some significance in determining air quality when using the AQI, but not necessarily in the way that we hypothesized. Perhaps use of a different metric of air pollution would change our outcomes; as seen in the introduction, many studies utilized PM2.5, or fine particulate air pollution, when testing racial/ethnic demographics effect on air pollution. These studies found much more statistical significance in how their features affect levels of PM2.5. In our model, the local date held much more significance than all racial and income demographics."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#concluding-discussion",
    "href": "posts/final_project_blog/final-project-blog-post.html#concluding-discussion",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nWhen we began this project, our main goals were to explore the relationships between income, racial demographics, and air pollution through the modelling process and hopefully gain insights into current inequalities surrounding pollution in the United States. We hoped that we would be able to implement and train a model to successfully complete binary predictions about ari quality based on the demographic data we selected from the United States Census.\nIn the end our model was able to predict air quality at a slightly higher rate than the base rate. One of the larger surprises of our project was that the statistical significance of median household income and racial demographics was somewhat low in predicting pollution. While it could be easy to lose perspective in the details of our project and feel disappointed in this result, when we take a step back and consider the implications for actual people in the United States this is a result we are happy with.\nAdditionally, we were quite successful in meeting the specific goals we defined at the beginning of this project. One of the challenges we initially identified that we were most worried about was the data cleaning process. While it did take a significant amount of time to gather the data we wanted to use, clean it, and combine data frames we were able to obtain training and test data with AQI, median income, and racial demographics by county. Our other initial goals included a working model that predicts whether a given county is a high pollution or low pollution area and a Jupyter Notebook with experimental graphics for both feature selection and our results. Both of these goals were met successfully and the feature selection process, which was something we thought might not happen if our plans did not go directly as planned, was an integral part of our project from which we gleaned many of the insights about our results.\nAs stated in both our introduction and the discussion of our results, there have been a number of studies that found Black Americans were more likely to live in heavily polluted areas and have health issues related to particle pollution. One hypothesis for why our results were less statistically significant than other similar work is that we were only considering urban pollution and that is not a comlete picture of life in the United States. There are many other historical legacies or redlining and discriminatory housing practices in suburbs that inform urban demographics, so if we had more time and access to data exploring rural areas would be an interesting extension of this project.\nIf we had more time we would also ideally consider features like poverty rates, unemployment rates, and income inequality in addition to median household income. Median household income is useful but there are many other things that contribute to the economic situation in a county beyond that and we would have loved to been able to compare how each were related to pollution rates. Finally, with more resources it would have been great to replicate our process considering PM2.5 instead of just AQI, the air quality index. The choice to use AQI was largely the result of what datasets were available to us but seeing how our model would perform in the context of a different measure of air pollution that has been shown to have racial disparities would be interesting and could potentially lead to an improved accuracy."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#group-contributions-statement",
    "href": "posts/final_project_blog/final-project-blog-post.html#group-contributions-statement",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nThis project was largely broken down into the following stages: data selection and preparation, data exploration, feature selection, model selection, and experimentation/visualizations. Beyond that, the major tasks were creating our repository and writing this final blog post.\nKate selected our initial EPA data for the project proposal and defined the overall AQI based on multiple pollutants in the data set. Mia worked on finding census data that had racial demographics by county and Bridget found a data set that had income levels as well. The three of us then worked together on various facets of the data cleaning process.\nBridget took the lead on our feature selection process and consequently decided what features would be included/dropped for our model training. She additionally created visualizations for the experimentation surrounding feature selection. Kate then did some initial data exploration that found some loose patterns through plotting and mapping our selected features. She started the mapping source code using geopandas and Mia continued this process and improved the user experience by making our maps interactive. Then it was time to actually train and test our models. Kate did the initial experimentation with multiple models and compared the results to select the best one. There was some experimentation throughout that process and Bridget helped with the details of trouble shooting. Mia helped with the cross validation scoring implementation. Once we had a model, Kate was able to map our predictions using geopandas. Bridget and Mia worked on the statistical significance of our different features and interpretting the results. Mia also performed an income and race audit of our model based on the racial audit we performed earlier in the semester for a blog post.\nWhen completing our project and writing the final blog post, Bridget authored our abstract, introduction, and values statement. Mia and Bridget both wrote parts of the results and methods section. Kate wrote the concluding discussion and parts of the experimentation and visualization section as well. We worked together to maintain our repository throughout the project."
  },
  {
    "objectID": "posts/final_project_blog/final-project-blog-post.html#personal-reflection",
    "href": "posts/final_project_blog/final-project-blog-post.html#personal-reflection",
    "title": "Urban Air Pollution Predicted by Income and Racial Demographics",
    "section": "Personal Reflection",
    "text": "Personal Reflection\n\nMia\nI learned a lot from this process. I contributed to most parts of this project and was able to gain insight into all of the methods. The data cleaning method allowed me to practice combining multiple data sets. For most projects, the data will not be readily available or clean. Data cleaning is a great skill to have and will allow the fine tuning and tailoring of the data. I was also able to use some of my knowledge from the earlier portion of the class and apply it to this project.\nI previously completed a blog post on classification models; I could use this knowledge for the implementation of different models using cross validation scores. I also learned to use the various Plotly features such as the geomapping choropleth functionality. This was a fun and important skill to learn, as we can know visualize our predictions and data. One of out other goals was to audit for different biases. We looked into the biases of income and racial demographics. I was able to practice some my statistical and analytical skills.\nAt the beginning of this project our goals were to compile a combined data set, a python package with our code and a final jupyter notebook. I believe that we have accomplished that. We have a combine dataset with racial demographic, income and EPA information. We also have crafted a jupyter notebook with all of our data cleaning, feature importance, classification modeling and bias auditing code. We also have this jupyter notebook with a more formal write-up of our process and results. The pieces of this project that I will carry forward are the collaboration aspects and the bias auditing. Now, I will think more critically about the biases, implications and dangers of future algorithms. The open communication and collaboration skills that I have improved throughout this process will also help me in my future endeavors.\n\n\nBridget\nI am surprised by how much I learned throughout the process beyond what I’ve learned in implementing blog posts and completing class warm-ups. A lot of the methodology required data cleaning and manipulating data sets, which is something I did not have a lot of experience with prior to the project. I also feel as though working in a team of three to delegate and complete different parts of the project was very valuable. It is easy to work in group projects and feel as though you did not complete enough, but with this project every aspect was equally as important and required the same amount of care. It also required very open communication with team members on what each of us were working on and pushing to GitHub.\nI feel as though we achieved a lot, particularly in learning, throughout this project. We met our initial goals of exploring how racial and income demographics affect urban air pollution, particularily the AQI. Perhaps what we hypothesized wasn’t the outcome, but we were still able to explore how demographics affect the AQI. I feel as though our group also met the initial goals we put forth of meeting to discuss goals, meeting with Phil if we felt as though we were stuck, and communicating about what we were all doing throughout the project.\nI am glad that I was able to work on a group project entirely directed by myself and my team before graduating college and moving into the workforce. I have worked on a few projects before, but working from raw datasets and creating my own datasets is not really something I have experimented with. I also feel as though working with three people is fairly unique; my other three large CS projects have been either in pairs or a much larger group. I feel as though it requires more thoughful delegation of tasks and understanding of one’s strengths and weaknesses. Overall I feel as though this project has taught me a lot about communicating with a team, working from brainstorm to implementation in a unique project, and how much work goes into machine learning projects prior to the actual model implementation,\n\n\nKate\nThroughout this project, I feel that we met our process and final goals. We started with the intention to have a final Jupyter Notebook displaying our results and experiments, a python repository, and an audit. We succeeded in producing all of those deliverables and as a result I feel satisfied with the work we produced collectively. On a personal level, I set goals at the beginning of the semester to work on a project I was passionate about and that had something to do with environmental issues. Additionally, in my initial goal setting I stated that I wanted to focus on experimentation and social responsibility throughout the course and project process. I am really happy with how I was able to align my interests and goals with our project topic and as a result I was working on something I was interested in and passionate about.\nI think I learned a lot through the data cleaning process and the experimentation aspects of the project. I was more familiar with implementation through other classes and working on a project that was so flexible and up to us helped me learn new skills and techniques for working with data that I think will be very useful moving forward into my career and life after Middlebury. Additionally, I think one skill I learned through this project was how to make decisions in the modelling process and more generally. Decision making is so fundamental to any aspect of machine learning, as we have seen throughout the semester, and being able to both make decisions without questioning myself and trust the decisions made by my teammates was a valuable skill I think I developed throughout this project.\nAs with any group project, I think working in a team for the latter half of the semester also improved my communication skills and let me learn my own strengths in the context of our project. I hope to carry both the concrete skills and the team work things I learned throughout this project beyond the course. I also think this project highlights that computing and machine learning projects are about real people and effect their lives, health outcomes, and more. That is a lesson from this class that I want to remain in the front of my mind as I move from an academic setting at Middlebury into the professional world."
  }
]